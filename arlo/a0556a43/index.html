<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"islocal.cc",root:"/",scheme:"Muse",version:"7.8.0",exturl:!1,sidebar:{position:"right",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:"default"},back2top:{enable:!0,sidebar:!1,scrollpercent:!1},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="Ceph介绍Ceph基础Ceph是一个可靠地、自动重均衡、自动恢复的分布式存储系统，根据场景划分可以将Ceph分为三大块，分别是对象存储（rgw）、块设备存储（ rbd）和文件系统服务（cephfs）。Ceph相比其它存储的优势点在于它不单单是存储，同时还充分利用了存储节点上的计算能力，在存储每一个数据时，都会通过计算得出该数据存储的位置，尽量将数据分布均衡，同时由于Ceph的良好设计，采用了CR"><meta property="og:type" content="article"><meta property="og:title" content="Ceph分布式存储系统搭建"><meta property="og:url" content="http://islocal.cc/arlo/a0556a43/index.html"><meta property="og:site_name" content="南山草舍"><meta property="og:description" content="Ceph介绍Ceph基础Ceph是一个可靠地、自动重均衡、自动恢复的分布式存储系统，根据场景划分可以将Ceph分为三大块，分别是对象存储（rgw）、块设备存储（ rbd）和文件系统服务（cephfs）。Ceph相比其它存储的优势点在于它不单单是存储，同时还充分利用了存储节点上的计算能力，在存储每一个数据时，都会通过计算得出该数据存储的位置，尽量将数据分布均衡，同时由于Ceph的良好设计，采用了CR"><meta property="og:locale" content="zh_CN"><meta property="article:published_time" content="2022-02-18T03:18:10.000Z"><meta property="article:modified_time" content="2022-02-18T07:53:54.000Z"><meta property="article:author" content="南山小樵"><meta property="article:tag" content="Ceph"><meta name="twitter:card" content="summary"><link rel="canonical" href="http://islocal.cc/arlo/a0556a43/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>Ceph分布式存储系统搭建 | 南山草舍</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript><link rel="stylesheet" href="/css/prism.css" type="text/css"></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">南山草舍</h1><span class="logo-line-after"><i></i></span></a></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">17</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">174</span></a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">101</span></a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="http://islocal.cc/arlo/a0556a43/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="南山小樵"><meta itemprop="description" content=""></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="南山草舍"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Ceph分布式存储系统搭建</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2022-02-18 11:18:10 / 修改时间：15:53:54" itemprop="dateCreated datePublished" datetime="2022-02-18T11:18:10+08:00">2022-02-18</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%90%AD%E5%BB%BA/" itemprop="url" rel="index"><span itemprop="name">服务器搭建</span></a></span></span></div></header><div class="post-body" itemprop="articleBody"><h1 id="Ceph介绍"><a href="#Ceph介绍" class="headerlink" title="Ceph介绍"></a>Ceph介绍</h1><h2 id="Ceph基础"><a href="#Ceph基础" class="headerlink" title="Ceph基础"></a>Ceph基础</h2><p>Ceph是一个可靠地、自动重均衡、自动恢复的分布式存储系统，根据场景划分可以将Ceph分为三大块，分别是对象存储（rgw）、块设备存储（ rbd）和文件系统服务（cephfs）。Ceph相比其它存储的优势点在于它不单单是存储，同时还充分利用了存储节点上的计算能力，在存储每一个数据时，都会通过计算得出该数据存储的位置，尽量将数据分布均衡，同时由于Ceph的良好设计，采用了CRUSH算法、HASH环等方法，使得它不存在传统的单点故障的问题，且随着规模的扩大性能并不会受到影响。</p><span id="more"></span><h2 id="Ceph核心组件"><a href="#Ceph核心组件" class="headerlink" title="Ceph核心组件"></a>Ceph核心组件</h2><p>Ceph的核心组件包括Ceph OSD、Ceph Monitor和Ceph MDS。</p><p><strong>Ceph OSD</strong>：OSD的英文全称是Object Storage Device，它的主要功能是存储数据、复制数据、平衡数据、恢复数据等，与其它OSD间进行心跳检查等，并将一些变化情况上报给Ceph Monitor。一般情况下一块硬盘对应一个OSD，由OSD来对硬盘存储进行管理，当然一个分区也可以成为一个OSD。</p><p>Ceph OSD的架构实现由物理磁盘驱动器、Linux文件系统和Ceph OSD服务组成，对于Ceph OSD Deamon而言，Linux文件系统显性的支持了其拓展性，一般Linux文件系统有好几种，比如有BTRFS、XFS、Ext4等，BTRFS虽然有很多优点特性，但现在还没达到生产环境所需的稳定性，一般比较推荐使用XFS。</p><p>伴随OSD的还有一个概念叫做Journal盘，一般写数据到Ceph集群时，都是先将数据写入到Journal盘中，然后每隔一段时间比如5秒再将Journal盘中的数据刷新到文件系统中。一般为了使读写时延更小，Journal盘都是采用SSD，一般分配10G以上，当然分配多点那是更好，Ceph中引入Journal盘的概念是因为Journal允许Ceph OSD功能很快做小的写操作；一个随机写入首先写入在上一个连续类型的journal，然后刷新到文件系统，这给了文件系统足够的时间来合并写入磁盘，一般情况下使用SSD作为OSD的journal可以有效缓冲突发负载。</p><p><strong>Ceph Monitor</strong>：由该英文名字我们可以知道它是一个监视器，负责监视Ceph集群，维护Ceph集群的健康状态，同时维护着Ceph集群中的各种Map图，比如OSD Map、Monitor Map、PG Map和CRUSH Map，这些Map统称为Cluster Map，Cluster Map是RADOS的关键数据结构，管理集群中的所有成员、关系、属性等信息以及数据的分发，比如当用户需要存储数据到Ceph集群时，OSD需要先通过Monitor获取最新的Map图，然后根据Map图和object id等计算出数据最终存储的位置。</p><p><strong>Ceph MDS</strong>：全称是Ceph MetaData Server，主要保存的文件系统服务的元数据，但对象存储和块存储设备是不需要使用该服务的。</p><p>查看各种Map的信息可以通过如下命令：ceph osd(mon、pg) dump</p><h1 id="规划"><a href="#规划" class="headerlink" title="规划"></a>规划</h1><table><thead><tr><th>主机名</th><th>IP地址</th><th>角色</th><th>配置</th></tr></thead><tbody><tr><td>ceph_node1</td><td>192.168.111.11&#x2F;24</td><td>控制节点、mon，mgr，osd</td><td>3块10GB硬盘</td></tr><tr><td>ceph_node2</td><td>192.168.111.12&#x2F;24</td><td>mon, mgr, osd</td><td>3块10GB硬盘</td></tr><tr><td>ceph_node3</td><td>192.168.111.13&#x2F;24</td><td>mon, mgr, osd</td><td>3块10GB硬盘</td></tr></tbody></table><h1 id="系统初始化"><a href="#系统初始化" class="headerlink" title="系统初始化"></a>系统初始化</h1><h2 id="关闭防火墙"><a href="#关闭防火墙" class="headerlink" title="关闭防火墙"></a>关闭防火墙</h2><p><code>systemctl stop firewalld &amp;&amp; systemctl disable firewalld</code></p><h2 id="关闭SELinux"><a href="#关闭SELinux" class="headerlink" title="关闭SELinux"></a>关闭SELinux</h2><p>在三台主机上分别执行以下命令，管理SELinux</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sed -i &#x27;s/SELINUX=enforcing/SELINUX=disabled/&#x27; /etc/selinux/config</span><br><span class="line">grep SELINUX=disabled /etc/selinux/config</span><br><span class="line">setenforce 0</span><br><span class="line">getenforce </span><br></pre></td></tr></table></figure><h2 id="配置网络"><a href="#配置网络" class="headerlink" title="配置网络"></a>配置网络</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">#ceph_node1 </span><br><span class="line">nmcli connection delete ens33</span><br><span class="line">nmcli connection add con-name ens33 ifname ens33 autoconnect yes type ethernet ip4 192.168.111.11/24 gw4 192.168.111.254 ipv4.dns 223.5.5.5</span><br><span class="line">nmcli conn up ens33</span><br><span class="line"></span><br><span class="line">#ceph_node2</span><br><span class="line">nmcli connection delete ens33</span><br><span class="line">nmcli connection add con-name ens33 ifname ens33 autoconnect yes type ethernet ip4 192.168.111.12/24 gw4 192.168.111.254 ipv4.dns 223.5.5.5</span><br><span class="line">nmcli conn up ens33</span><br><span class="line"></span><br><span class="line">#ceph_node3</span><br><span class="line">nmcli connection delete ens33</span><br><span class="line">nmcli connection add con-name ens33 ifname ens33 autoconnect yes type ethernet ip4 192.168.111.13/24 gw4 192.168.111.254 ipv4.dns 223.5.5.5</span><br><span class="line">nmcli conn up ens33</span><br></pre></td></tr></table></figure><h2 id="配置主机名"><a href="#配置主机名" class="headerlink" title="配置主机名"></a>配置主机名</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#ceph_node1 </span><br><span class="line">hostnamectl set-hostname ceph_node1</span><br><span class="line"></span><br><span class="line">#ceph_node2</span><br><span class="line">hostnamectl set-hostname ceph_node2</span><br><span class="line"></span><br><span class="line">#ceph_node3</span><br><span class="line">hostnamectl set-hostname ceph_node3</span><br></pre></td></tr></table></figure><h2 id="配置hosts"><a href="#配置hosts" class="headerlink" title="配置hosts"></a>配置hosts</h2><p>配置hosts，并复制到其他两台主机</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#ceph_node1</span><br><span class="line"></span><br><span class="line">vi /etc/hosts</span><br><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line">192.168.111.11 ceph_node1</span><br><span class="line">192.168.111.12 ceph_node2</span><br><span class="line">192.168.111.13 ceph_node3</span><br></pre></td></tr></table></figure><p><code>scp /etc/hosts ceph_node2:/etc/</code></p><p><code>scp /etc/hosts ceph_node3:/etc/</code></p><h2 id="配置互信"><a href="#配置互信" class="headerlink" title="配置互信"></a>配置互信</h2><p>在ceph_node1上生成秘钥，不设置密码</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# ssh-keygen </span><br><span class="line">Generating public/private rsa key pair.</span><br><span class="line">Enter file in which to save the key (/root/.ssh/id_rsa): </span><br><span class="line">Enter passphrase (empty for no passphrase): </span><br><span class="line">Enter same passphrase again: </span><br><span class="line">Your identification has been saved in /root/.ssh/id_rsa.</span><br><span class="line">Your public key has been saved in /root/.ssh/id_rsa.pub.</span><br><span class="line">The key fingerprint is:</span><br><span class="line">SHA256:pU9BBVdNpHdTAXToafsymcxRpbLdCTpX/ivZZ5U18lQ root@ceph_node1</span><br><span class="line">The key&#x27;s randomart image is:</span><br><span class="line">+---[RSA 2048]----+</span><br><span class="line">|          oo+++*=|</span><br><span class="line">|         . . ...E|</span><br><span class="line">|          o . oo=|</span><br><span class="line">|         o ..*.*+|</span><br><span class="line">|        S . o+X.=|</span><br><span class="line">|         o o.+.=o|</span><br><span class="line">|          . = B o|</span><br><span class="line">|             X o+|</span><br><span class="line">|              +oo|</span><br><span class="line">+----[SHA256]-----+</span><br></pre></td></tr></table></figure><p>将秘钥分别复制到ceph_node1, ceph_node2, ceph_node3上</p><p><code>ssh-copy-id -i /root/.ssh/id_rsa.pub ceph_node1</code><br><code>ssh-copy-id -i /root/.ssh/id_rsa.pub ceph_node2</code><br><code>ssh-copy-id -i /root/.ssh/id_rsa.pub ceph_node3</code></p><p>验证一下免密效果</p><p><code>ssh ceph_node1</code></p><p><code>ssh ceph_node2</code></p><p><code>ssh ceph_node3</code></p><h2 id="配置时间服务器"><a href="#配置时间服务器" class="headerlink" title="配置时间服务器"></a>配置时间服务器</h2><p>在ceph_node1上配置时间服务器</p><p>安装chrony时间服务器软件</p><p><code>yum -y install chrony</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/chrony.conf</span><br><span class="line"></span><br><span class="line">server ntp1.aliyun.com iburst</span><br><span class="line">server ntp2.aliyun.com iburst</span><br><span class="line">server ntp3.aliyun.com iburst</span><br><span class="line"></span><br><span class="line">allow 192.168.111.0/24</span><br><span class="line">local stratum 10</span><br></pre></td></tr></table></figure><p><code>systemctl start chronyd &amp;&amp; systemctl enable chronyd</code></p><p>在ceph_node2 &amp; ceph_node3上配置</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/chrony.conf</span><br><span class="line"></span><br><span class="line">server ceph_node1 iburst</span><br></pre></td></tr></table></figure><p>查看时间同步源</p><p><code>chronyc sources -v</code></p><p>手动同步系统时钟</p><p><code>chronyc -a makestep</code></p><h2 id="配置yum源"><a href="#配置yum源" class="headerlink" title="配置yum源"></a>配置yum源</h2><p><code>cd /etc/yum.repos.d/</code></p><p><code>mkdir bak</code></p><p><code>mv *.repo bak</code></p><h3 id="基础yum源"><a href="#基础yum源" class="headerlink" title="基础yum源"></a>基础yum源</h3><p><code>curl -o /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo</code></p><h3 id="epel源"><a href="#epel源" class="headerlink" title="epel源"></a>epel源</h3><p><code>curl -o /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo</code></p><h3 id="ceph源"><a href="#ceph源" class="headerlink" title="ceph源"></a>ceph源</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF |tee /etc/yum.repos.d/ceph.repo</span><br><span class="line"></span><br><span class="line">[Ceph]</span><br><span class="line">name=Ceph packages for $basearch</span><br><span class="line">baseurl=https://mirrors.aliyun.com/ceph/rpm-nautilus/el7/\$basearch</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">type=rpm-md</span><br><span class="line">gpgkey=https://download.ceph.com/keys/release.asc</span><br><span class="line">priority=1</span><br><span class="line"></span><br><span class="line">[Ceph-noarch]</span><br><span class="line">name=Ceph noarch packages </span><br><span class="line">baseurl=https://mirrors.aliyun.com/ceph/rpm-nautilus/el7/noarch</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">type=rpm-md</span><br><span class="line">gpgkey=https://download.ceph.com/keys/release.asc</span><br><span class="line">priority=1</span><br><span class="line"></span><br><span class="line">[Ceph-source]</span><br><span class="line">name=Ceph source packages </span><br><span class="line">baseurl=https://mirrors.aliyun.com/ceph/rpm-nautilus/el7/SRPMS</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">type=rpm-md</span><br><span class="line">gpgkey=https://download.ceph.com/keys/release.asc</span><br><span class="line"></span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>生成yum源缓存</p><p><code>yum clean all</code></p><p><code>yum makecache fast</code></p><p>yum list |grep ceph</p><h1 id="部署Ceph"><a href="#部署Ceph" class="headerlink" title="部署Ceph"></a>部署Ceph</h1><h2 id="部署控制节点"><a href="#部署控制节点" class="headerlink" title="部署控制节点"></a>部署控制节点</h2><p>在ceph_node1节点上安装ceph-deploy</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install ceph-deploy ceph </span><br></pre></td></tr></table></figure><p>在ceph_node1上创建一个cluster目录，<strong>所有命令再此目录下进行操作</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph_node1 ~]# mkdir /cluster</span><br><span class="line">[root@ceph_node1 ~]# cd /cluster</span><br></pre></td></tr></table></figure><p>将ceph_node1,ceph_node2,ceph_node3加入集群</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph_node1 cluster]# ceph-deploy new ceph_node1 ceph_node2 ceph_node3</span><br><span class="line"></span><br><span class="line">[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf</span><br><span class="line">[ceph_deploy.cli][INFO  ] Invoked (2.0.1): /usr/bin/ceph-deploy new ceph_node1 ceph_node2 ceph_node3</span><br><span class="line">[ceph_deploy.cli][INFO  ] ceph-deploy options:</span><br><span class="line">[ceph_deploy.cli][INFO  ]  username                      : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  func                          : &lt;function new at 0x7ffbbbc21de8&gt;</span><br><span class="line">[ceph_deploy.cli][INFO  ]  verbose                       : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  overwrite_conf                : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  quiet                         : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7ffbbb39c4d0&gt;</span><br><span class="line">[ceph_deploy.cli][INFO  ]  cluster                       : ceph</span><br><span class="line">[ceph_deploy.cli][INFO  ]  ssh_copykey                   : True</span><br><span class="line">[ceph_deploy.cli][INFO  ]  mon                           : [&#x27;ceph_node1&#x27;, &#x27;ceph_node2&#x27;, &#x27;ceph_node3&#x27;]</span><br><span class="line">[ceph_deploy.cli][INFO  ]  public_network                : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  ceph_conf                     : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  cluster_network               : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  default_release               : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  fsid                          : None</span><br><span class="line">[ceph_deploy.new][DEBUG ] Creating new cluster named ceph</span><br><span class="line">[ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds</span><br><span class="line">[ceph_node1][DEBUG ] connected to host: ceph_node1 </span><br><span class="line">[ceph_node1][DEBUG ] detect platform information from remote host</span><br><span class="line">[ceph_node1][DEBUG ] detect machine type</span><br><span class="line">[ceph_node1][DEBUG ] find the location of an executable</span><br><span class="line">[ceph_node1][INFO  ] Running command: /usr/sbin/ip link show</span><br><span class="line">[ceph_node1][INFO  ] Running command: /usr/sbin/ip addr show</span><br><span class="line">[ceph_node1][DEBUG ] IP addresses found: [u&#x27;192.168.111.11&#x27;]</span><br><span class="line">[ceph_deploy.new][DEBUG ] Resolving host ceph_node1</span><br><span class="line">[ceph_deploy.new][DEBUG ] Monitor ceph_node1 at 192.168.111.11</span><br><span class="line">[ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds</span><br><span class="line">[ceph_node2][DEBUG ] connected to host: ceph_node1 </span><br><span class="line">[ceph_node2][INFO  ] Running command: ssh -CT -o BatchMode=yes ceph_node2</span><br><span class="line">[ceph_node2][DEBUG ] connected to host: ceph_node2 </span><br><span class="line">[ceph_node2][DEBUG ] detect platform information from remote host</span><br><span class="line">[ceph_node2][DEBUG ] detect machine type</span><br><span class="line">[ceph_node2][DEBUG ] find the location of an executable</span><br><span class="line">[ceph_node2][INFO  ] Running command: /usr/sbin/ip link show</span><br><span class="line">[ceph_node2][INFO  ] Running command: /usr/sbin/ip addr show</span><br><span class="line">[ceph_node2][DEBUG ] IP addresses found: [u&#x27;192.168.111.12&#x27;]</span><br><span class="line">[ceph_deploy.new][DEBUG ] Resolving host ceph_node2</span><br><span class="line">[ceph_deploy.new][DEBUG ] Monitor ceph_node2 at 192.168.111.12</span><br><span class="line">[ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds</span><br><span class="line">[ceph_node3][DEBUG ] connected to host: ceph_node1 </span><br><span class="line">[ceph_node3][INFO  ] Running command: ssh -CT -o BatchMode=yes ceph_node3</span><br><span class="line">[ceph_node3][DEBUG ] connected to host: ceph_node3 </span><br><span class="line">[ceph_node3][DEBUG ] detect platform information from remote host</span><br><span class="line">[ceph_node3][DEBUG ] detect machine type</span><br><span class="line">[ceph_node3][DEBUG ] find the location of an executable</span><br><span class="line">[ceph_node3][INFO  ] Running command: /usr/sbin/ip link show</span><br><span class="line">[ceph_node3][INFO  ] Running command: /usr/sbin/ip addr show</span><br><span class="line">[ceph_node3][DEBUG ] IP addresses found: [u&#x27;192.168.111.13&#x27;]</span><br><span class="line">[ceph_deploy.new][DEBUG ] Resolving host ceph_node3</span><br><span class="line">[ceph_deploy.new][DEBUG ] Monitor ceph_node3 at 192.168.111.13</span><br><span class="line">[ceph_deploy.new][DEBUG ] Monitor initial members are [&#x27;ceph_node1&#x27;, &#x27;ceph_node2&#x27;, &#x27;ceph_node3&#x27;]</span><br><span class="line">[ceph_deploy.new][DEBUG ] Monitor addrs are [&#x27;192.168.111.11&#x27;, &#x27;192.168.111.12&#x27;, &#x27;192.168.111.13&#x27;]</span><br><span class="line">[ceph_deploy.new][DEBUG ] Creating a random mon key...</span><br><span class="line">[ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...</span><br><span class="line">[ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...</span><br></pre></td></tr></table></figure><p>输出没有报错，表示部署成功。</p><p>查看ceph版本</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph_node1 cluster]# ceph -v</span><br><span class="line">ceph version 14.2.22 (ca74598065096e6fcbd8433c8779a2be0c889351) nautilus (stable)</span><br></pre></td></tr></table></figure><h2 id="生成mon角色"><a href="#生成mon角色" class="headerlink" title="生成mon角色"></a>生成mon角色</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph_node1 cluster]# ceph-deploy mon create-initial</span><br><span class="line"></span><br><span class="line">[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf</span><br><span class="line">[ceph_deploy.cli][INFO  ] Invoked (2.0.1): /usr/bin/ceph-deploy mon create-initial</span><br><span class="line">[ceph_deploy.cli][INFO  ] ceph-deploy options:</span><br><span class="line">[ceph_deploy.cli][INFO  ]  username                      : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  verbose                       : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  overwrite_conf                : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  subcommand                    : create-initial</span><br><span class="line">[ceph_deploy.cli][INFO  ]  quiet                         : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7f8c09aa2e60&gt;</span><br><span class="line">[ceph_deploy.cli][INFO  ]  cluster                       : ceph</span><br><span class="line">[ceph_deploy.cli][INFO  ]  func                          : &lt;function mon at 0x7f8c09af7410&gt;</span><br><span class="line">[ceph_deploy.cli][INFO  ]  ceph_conf                     : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  default_release               : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  keyrings                      : None</span><br><span class="line">[ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts ceph_node1 ceph_node2 ceph_node3</span><br><span class="line">[ceph_deploy.mon][DEBUG ] detecting platform for host ceph_node1 ...</span><br><span class="line">[ceph_node1][DEBUG ] connected to host: ceph_node1 </span><br><span class="line">[ceph_node1][DEBUG ] detect platform information from remote host</span><br><span class="line">[ceph_node1][DEBUG ] detect machine type</span><br><span class="line">[ceph_node1][DEBUG ] find the location of an executable</span><br><span class="line">[ceph_deploy.mon][INFO  ] distro info: CentOS Linux 7.9.2009 Core</span><br><span class="line">[ceph_node1][DEBUG ] determining if provided host has same hostname in remote</span><br><span class="line">[ceph_node1][DEBUG ] get remote short hostname</span><br><span class="line">[ceph_node1][DEBUG ] deploying mon to ceph_node1</span><br><span class="line">[ceph_node1][DEBUG ] get remote short hostname</span><br><span class="line">[ceph_node1][DEBUG ] remote hostname: ceph_node1</span><br><span class="line">[ceph_node1][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf</span><br><span class="line">[ceph_node1][DEBUG ] create the mon path if it does not exist</span><br><span class="line">[ceph_node1][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-ceph_node1/done</span><br><span class="line">[ceph_node1][DEBUG ] create a done file to avoid re-doing the mon deployment</span><br><span class="line">[ceph_node1][DEBUG ] create the init path if it does not exist</span><br><span class="line">[ceph_node1][INFO  ] Running command: systemctl enable ceph.target</span><br><span class="line">[ceph_node1][INFO  ] Running command: systemctl enable ceph-mon@ceph_node1</span><br><span class="line">[ceph_node1][INFO  ] Running command: systemctl start ceph-mon@ceph_node1</span><br><span class="line">[ceph_node1][INFO  ] Running command: ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.ceph_node1.asok mon_status</span><br><span class="line">[ceph_node1][DEBUG ] ********************************************************************************</span><br><span class="line">[ceph_node1][DEBUG ] status for monitor: mon.ceph_node1</span><br><span class="line">[ceph_node1][DEBUG ] &#123;</span><br><span class="line">[ceph_node1][DEBUG ]   &quot;election_epoch&quot;: 6, </span><br><span class="line">[ceph_node1][DEBUG ]   &quot;extra_probe_peers&quot;: [</span><br><span class="line">[ceph_node1][DEBUG ]     &#123;</span><br><span class="line">[ceph_node1][DEBUG ]       &quot;addrvec&quot;: [</span><br><span class="line">[ceph_node1][DEBUG ]         &#123;</span><br><span class="line">[ceph_node1][DEBUG ]           &quot;addr&quot;: &quot;192.168.111.12:3300&quot;, </span><br><span class="line">[ceph_node1][DEBUG ]           &quot;nonce&quot;: 0, </span><br><span class="line">[ceph_node1][DEBUG ]           &quot;type&quot;: &quot;v2&quot;</span><br><span class="line">[ceph_node1][DEBUG ]         &#125;, </span><br><span class="line">[ceph_node1][DEBUG ]         &#123;</span><br><span class="line">[ceph_node1][DEBUG ]           &quot;addr&quot;: &quot;192.168.111.12:6789&quot;, </span><br><span class="line">[ceph_node1][DEBUG ]           &quot;nonce&quot;: 0, </span><br><span class="line">[ceph_node1][DEBUG ]           &quot;type&quot;: &quot;v1&quot;</span><br><span class="line">[ceph_node1][DEBUG ]         &#125;</span><br><span class="line">[ceph_node1][DEBUG ]       ]</span><br><span class="line">[ceph_node1][DEBUG ]     &#125;, </span><br><span class="line">[ceph_node1][DEBUG ]     &#123;</span><br><span class="line">[ceph_node1][DEBUG ]       &quot;addrvec&quot;: [</span><br><span class="line">[ceph_node1][DEBUG ]         &#123;</span><br><span class="line">[ceph_node1][DEBUG ]           &quot;addr&quot;: &quot;192.168.111.13:3300&quot;, </span><br><span class="line">[ceph_node1][DEBUG ]           &quot;nonce&quot;: 0, </span><br><span class="line">[ceph_node1][DEBUG ]           &quot;type&quot;: &quot;v2&quot;</span><br><span class="line">[ceph_node1][DEBUG ]         &#125;, </span><br><span class="line">[ceph_node1][DEBUG ]         &#123;</span><br><span class="line">[ceph_node1][DEBUG ]           &quot;addr&quot;: &quot;192.168.111.13:6789&quot;, </span><br><span class="line">[ceph_node1][DEBUG ]           &quot;nonce&quot;: 0, </span><br><span class="line">[ceph_node1][DEBUG ]           &quot;type&quot;: &quot;v1&quot;</span><br><span class="line">[ceph_node1][DEBUG ]         &#125;</span><br><span class="line">[ceph_node1][DEBUG ]       ]</span><br><span class="line">[ceph_node1][DEBUG ]     &#125;</span><br><span class="line">[ceph_node1][DEBUG ]   ], </span><br><span class="line">[ceph_node1][DEBUG ]   &quot;feature_map&quot;: &#123;</span><br><span class="line">[ceph_node1][DEBUG ]     &quot;mon&quot;: [</span><br><span class="line">[ceph_node1][DEBUG ]       &#123;</span><br><span class="line">[ceph_node1][DEBUG ]         &quot;features&quot;: &quot;0x3ffddff8ffecffff&quot;, </span><br><span class="line">[ceph_node1][DEBUG ]         &quot;num&quot;: 1, </span><br><span class="line">[ceph_node1][DEBUG ]         &quot;release&quot;: &quot;luminous&quot;</span><br><span class="line">[ceph_node1][DEBUG ]       &#125;</span><br><span class="line">[ceph_node1][DEBUG ]     ]</span><br><span class="line">[ceph_node1][DEBUG ]   &#125;, </span><br><span class="line">[ceph_node1][DEBUG ]   &quot;features&quot;: &#123;</span><br><span class="line">[ceph_node1][DEBUG ]     &quot;quorum_con&quot;: &quot;4611087854035861503&quot;, </span><br><span class="line">[ceph_node1][DEBUG ]     &quot;quorum_mon&quot;: [</span><br><span class="line">[ceph_node1][DEBUG ]       &quot;kraken&quot;, </span><br><span class="line">[ceph_node1][DEBUG ]       &quot;luminous&quot;, </span><br><span class="line">[ceph_node1][DEBUG ]       &quot;mimic&quot;, </span><br><span class="line">[ceph_node1][DEBUG ]       &quot;osdmap-prune&quot;, </span><br><span class="line">[ceph_node1][DEBUG ]       &quot;nautilus&quot;</span><br><span class="line">[ceph_node1][DEBUG ]     ], </span><br><span class="line">[ceph_node1][DEBUG ]     &quot;required_con&quot;: &quot;2449958747315912708&quot;, </span><br><span class="line">[ceph_node1][DEBUG ]     &quot;required_mon&quot;: [</span><br><span class="line">[ceph_node1][DEBUG ]       &quot;kraken&quot;, </span><br><span class="line">[ceph_node1][DEBUG ]       &quot;luminous&quot;, </span><br><span class="line">[ceph_node1][DEBUG ]       &quot;mimic&quot;, </span><br><span class="line">[ceph_node1][DEBUG ]       &quot;osdmap-prune&quot;, </span><br><span class="line">[ceph_node1][DEBUG ]       &quot;nautilus&quot;</span><br><span class="line">[ceph_node1][DEBUG ]     ]</span><br><span class="line">[ceph_node1][DEBUG ]   &#125;, </span><br><span class="line">[ceph_node1][DEBUG ]   &quot;monmap&quot;: &#123;</span><br><span class="line">[ceph_node1][DEBUG ]     &quot;created&quot;: &quot;2022-02-18 13:54:10.100119&quot;, </span><br><span class="line">[ceph_node1][DEBUG ]     &quot;epoch&quot;: 1, </span><br><span class="line">[ceph_node1][DEBUG ]     &quot;features&quot;: &#123;</span><br><span class="line">[ceph_node1][DEBUG ]       &quot;optional&quot;: [], </span><br><span class="line">[ceph_node1][DEBUG ]       &quot;persistent&quot;: [</span><br><span class="line">[ceph_node1][DEBUG ]         &quot;kraken&quot;, </span><br><span class="line">[ceph_node1][DEBUG ]         &quot;luminous&quot;, </span><br><span class="line">[ceph_node1][DEBUG ]         &quot;mimic&quot;, </span><br><span class="line">[ceph_node1][DEBUG ]         &quot;osdmap-prune&quot;, </span><br><span class="line">[ceph_node1][DEBUG ]         &quot;nautilus&quot;</span><br><span class="line">[ceph_node1][DEBUG ]       ]</span><br><span class="line">[ceph_node1][DEBUG ]     &#125;, </span><br><span class="line">[ceph_node1][DEBUG ]     &quot;fsid&quot;: &quot;f7be9981-1e57-4165-989e-3c7206331a20&quot;, </span><br><span class="line">[ceph_node1][DEBUG ]     &quot;min_mon_release&quot;: 14, </span><br><span class="line">[ceph_node1][DEBUG ]     &quot;min_mon_release_name&quot;: &quot;nautilus&quot;, </span><br><span class="line">[ceph_node1][DEBUG ]     &quot;modified&quot;: &quot;2022-02-18 13:54:10.100119&quot;, </span><br><span class="line">[ceph_node1][DEBUG ]     &quot;mons&quot;: [</span><br><span class="line">[ceph_node1][DEBUG ]       &#123;</span><br><span class="line">[ceph_node1][DEBUG ]         &quot;addr&quot;: &quot;192.168.111.11:6789/0&quot;, </span><br><span class="line">[ceph_node1][DEBUG ]         &quot;name&quot;: &quot;ceph_node1&quot;, </span><br><span class="line">[ceph_node1][DEBUG ]         &quot;public_addr&quot;: &quot;192.168.111.11:6789/0&quot;, </span><br><span class="line">[ceph_node1][DEBUG ]         &quot;public_addrs&quot;: &#123;</span><br><span class="line">[ceph_node1][DEBUG ]           &quot;addrvec&quot;: [</span><br><span class="line">[ceph_node1][DEBUG ]             &#123;</span><br><span class="line">[ceph_node1][DEBUG ]               &quot;addr&quot;: &quot;192.168.111.11:3300&quot;, </span><br><span class="line">[ceph_node1][DEBUG ]               &quot;nonce&quot;: 0, </span><br><span class="line">[ceph_node1][DEBUG ]               &quot;type&quot;: &quot;v2&quot;</span><br><span class="line">[ceph_node1][DEBUG ]             &#125;, </span><br><span class="line">[ceph_node1][DEBUG ]             &#123;</span><br><span class="line">[ceph_node1][DEBUG ]               &quot;addr&quot;: &quot;192.168.111.11:6789&quot;, </span><br><span class="line">[ceph_node1][DEBUG ]               &quot;nonce&quot;: 0, </span><br><span class="line">[ceph_node1][DEBUG ]               &quot;type&quot;: &quot;v1&quot;</span><br><span class="line">[ceph_node1][DEBUG ]             &#125;</span><br><span class="line">[ceph_node1][DEBUG ]           ]</span><br><span class="line">[ceph_node1][DEBUG ]         &#125;, </span><br><span class="line">[ceph_node1][DEBUG ]         &quot;rank&quot;: 0</span><br><span class="line">[ceph_node1][DEBUG ]       &#125;, </span><br><span class="line">[ceph_node1][DEBUG ]       &#123;</span><br><span class="line">[ceph_node1][DEBUG ]         &quot;addr&quot;: &quot;192.168.111.12:6789/0&quot;, </span><br><span class="line">[ceph_node1][DEBUG ]         &quot;name&quot;: &quot;ceph_node2&quot;, </span><br><span class="line">[ceph_node1][DEBUG ]         &quot;public_addr&quot;: &quot;192.168.111.12:6789/0&quot;, </span><br><span class="line">[ceph_node1][DEBUG ]         &quot;public_addrs&quot;: &#123;</span><br><span class="line">[ceph_node1][DEBUG ]           &quot;addrvec&quot;: [</span><br><span class="line">[ceph_node1][DEBUG ]             &#123;</span><br><span class="line">[ceph_node1][DEBUG ]               &quot;addr&quot;: &quot;192.168.111.12:3300&quot;, </span><br><span class="line">[ceph_node1][DEBUG ]               &quot;nonce&quot;: 0, </span><br><span class="line">[ceph_node1][DEBUG ]               &quot;type&quot;: &quot;v2&quot;</span><br><span class="line">[ceph_node1][DEBUG ]             &#125;, </span><br><span class="line">[ceph_node1][DEBUG ]             &#123;</span><br><span class="line">[ceph_node1][DEBUG ]               &quot;addr&quot;: &quot;192.168.111.12:6789&quot;, </span><br><span class="line">[ceph_node1][DEBUG ]               &quot;nonce&quot;: 0, </span><br><span class="line">[ceph_node1][DEBUG ]               &quot;type&quot;: &quot;v1&quot;</span><br><span class="line">[ceph_node1][DEBUG ]             &#125;</span><br><span class="line">[ceph_node1][DEBUG ]           ]</span><br><span class="line">[ceph_node1][DEBUG ]         &#125;, </span><br><span class="line">[ceph_node1][DEBUG ]         &quot;rank&quot;: 1</span><br><span class="line">[ceph_node1][DEBUG ]       &#125;, </span><br><span class="line">[ceph_node1][DEBUG ]       &#123;</span><br><span class="line">[ceph_node1][DEBUG ]         &quot;addr&quot;: &quot;192.168.111.13:6789/0&quot;, </span><br><span class="line">[ceph_node1][DEBUG ]         &quot;name&quot;: &quot;ceph_node3&quot;, </span><br><span class="line">[ceph_node1][DEBUG ]         &quot;public_addr&quot;: &quot;192.168.111.13:6789/0&quot;, </span><br><span class="line">[ceph_node1][DEBUG ]         &quot;public_addrs&quot;: &#123;</span><br><span class="line">[ceph_node1][DEBUG ]           &quot;addrvec&quot;: [</span><br><span class="line">[ceph_node1][DEBUG ]             &#123;</span><br><span class="line">[ceph_node1][DEBUG ]               &quot;addr&quot;: &quot;192.168.111.13:3300&quot;, </span><br><span class="line">[ceph_node1][DEBUG ]               &quot;nonce&quot;: 0, </span><br><span class="line">[ceph_node1][DEBUG ]               &quot;type&quot;: &quot;v2&quot;</span><br><span class="line">[ceph_node1][DEBUG ]             &#125;, </span><br><span class="line">[ceph_node1][DEBUG ]             &#123;</span><br><span class="line">[ceph_node1][DEBUG ]               &quot;addr&quot;: &quot;192.168.111.13:6789&quot;, </span><br><span class="line">[ceph_node1][DEBUG ]               &quot;nonce&quot;: 0, </span><br><span class="line">[ceph_node1][DEBUG ]               &quot;type&quot;: &quot;v1&quot;</span><br><span class="line">[ceph_node1][DEBUG ]             &#125;</span><br><span class="line">[ceph_node1][DEBUG ]           ]</span><br><span class="line">[ceph_node1][DEBUG ]         &#125;, </span><br><span class="line">[ceph_node1][DEBUG ]         &quot;rank&quot;: 2</span><br><span class="line">[ceph_node1][DEBUG ]       &#125;</span><br><span class="line">[ceph_node1][DEBUG ]     ]</span><br><span class="line">[ceph_node1][DEBUG ]   &#125;, </span><br><span class="line">[ceph_node1][DEBUG ]   &quot;name&quot;: &quot;ceph_node1&quot;, </span><br><span class="line">[ceph_node1][DEBUG ]   &quot;outside_quorum&quot;: [], </span><br><span class="line">[ceph_node1][DEBUG ]   &quot;quorum&quot;: [</span><br><span class="line">[ceph_node1][DEBUG ]     0, </span><br><span class="line">[ceph_node1][DEBUG ]     1, </span><br><span class="line">[ceph_node1][DEBUG ]     2</span><br><span class="line">[ceph_node1][DEBUG ]   ], </span><br><span class="line">[ceph_node1][DEBUG ]   &quot;quorum_age&quot;: 34, </span><br><span class="line">[ceph_node1][DEBUG ]   &quot;rank&quot;: 0, </span><br><span class="line">[ceph_node1][DEBUG ]   &quot;state&quot;: &quot;leader&quot;, </span><br><span class="line">[ceph_node1][DEBUG ]   &quot;sync_provider&quot;: []</span><br><span class="line">[ceph_node1][DEBUG ] &#125;</span><br><span class="line">[ceph_node1][DEBUG ] ********************************************************************************</span><br><span class="line">[ceph_node1][INFO  ] monitor: mon.ceph_node1 is running</span><br><span class="line">[ceph_node1][INFO  ] Running command: ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.ceph_node1.asok mon_status</span><br><span class="line">[ceph_deploy.mon][DEBUG ] detecting platform for host ceph_node2 ...</span><br><span class="line">[ceph_node2][DEBUG ] connected to host: ceph_node2 </span><br><span class="line">[ceph_node2][DEBUG ] detect platform information from remote host</span><br><span class="line">[ceph_node2][DEBUG ] detect machine type</span><br><span class="line">[ceph_node2][DEBUG ] find the location of an executable</span><br><span class="line">[ceph_deploy.mon][INFO  ] distro info: CentOS Linux 7.9.2009 Core</span><br><span class="line">[ceph_node2][DEBUG ] determining if provided host has same hostname in remote</span><br><span class="line">[ceph_node2][DEBUG ] get remote short hostname</span><br><span class="line">[ceph_node2][DEBUG ] deploying mon to ceph_node2</span><br><span class="line">[ceph_node2][DEBUG ] get remote short hostname</span><br><span class="line">[ceph_node2][DEBUG ] remote hostname: ceph_node2</span><br><span class="line">[ceph_node2][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf</span><br><span class="line">[ceph_node2][DEBUG ] create the mon path if it does not exist</span><br><span class="line">[ceph_node2][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-ceph_node2/done</span><br><span class="line">[ceph_node2][DEBUG ] create a done file to avoid re-doing the mon deployment</span><br><span class="line">[ceph_node2][DEBUG ] create the init path if it does not exist</span><br><span class="line">[ceph_node2][INFO  ] Running command: systemctl enable ceph.target</span><br><span class="line">[ceph_node2][INFO  ] Running command: systemctl enable ceph-mon@ceph_node2</span><br><span class="line">[ceph_node2][INFO  ] Running command: systemctl start ceph-mon@ceph_node2</span><br><span class="line">[ceph_node2][INFO  ] Running command: ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.ceph_node2.asok mon_status</span><br><span class="line">[ceph_node2][DEBUG ] ********************************************************************************</span><br><span class="line">[ceph_node2][DEBUG ] status for monitor: mon.ceph_node2</span><br><span class="line">[ceph_node2][DEBUG ] &#123;</span><br><span class="line">[ceph_node2][DEBUG ]   &quot;election_epoch&quot;: 6, </span><br><span class="line">[ceph_node2][DEBUG ]   &quot;extra_probe_peers&quot;: [</span><br><span class="line">[ceph_node2][DEBUG ]     &#123;</span><br><span class="line">[ceph_node2][DEBUG ]       &quot;addrvec&quot;: [</span><br><span class="line">[ceph_node2][DEBUG ]         &#123;</span><br><span class="line">[ceph_node2][DEBUG ]           &quot;addr&quot;: &quot;192.168.111.11:3300&quot;, </span><br><span class="line">[ceph_node2][DEBUG ]           &quot;nonce&quot;: 0, </span><br><span class="line">[ceph_node2][DEBUG ]           &quot;type&quot;: &quot;v2&quot;</span><br><span class="line">[ceph_node2][DEBUG ]         &#125;, </span><br><span class="line">[ceph_node2][DEBUG ]         &#123;</span><br><span class="line">[ceph_node2][DEBUG ]           &quot;addr&quot;: &quot;192.168.111.11:6789&quot;, </span><br><span class="line">[ceph_node2][DEBUG ]           &quot;nonce&quot;: 0, </span><br><span class="line">[ceph_node2][DEBUG ]           &quot;type&quot;: &quot;v1&quot;</span><br><span class="line">[ceph_node2][DEBUG ]         &#125;</span><br><span class="line">[ceph_node2][DEBUG ]       ]</span><br><span class="line">[ceph_node2][DEBUG ]     &#125;, </span><br><span class="line">[ceph_node2][DEBUG ]     &#123;</span><br><span class="line">[ceph_node2][DEBUG ]       &quot;addrvec&quot;: [</span><br><span class="line">[ceph_node2][DEBUG ]         &#123;</span><br><span class="line">[ceph_node2][DEBUG ]           &quot;addr&quot;: &quot;192.168.111.13:3300&quot;, </span><br><span class="line">[ceph_node2][DEBUG ]           &quot;nonce&quot;: 0, </span><br><span class="line">[ceph_node2][DEBUG ]           &quot;type&quot;: &quot;v2&quot;</span><br><span class="line">[ceph_node2][DEBUG ]         &#125;, </span><br><span class="line">[ceph_node2][DEBUG ]         &#123;</span><br><span class="line">[ceph_node2][DEBUG ]           &quot;addr&quot;: &quot;192.168.111.13:6789&quot;, </span><br><span class="line">[ceph_node2][DEBUG ]           &quot;nonce&quot;: 0, </span><br><span class="line">[ceph_node2][DEBUG ]           &quot;type&quot;: &quot;v1&quot;</span><br><span class="line">[ceph_node2][DEBUG ]         &#125;</span><br><span class="line">[ceph_node2][DEBUG ]       ]</span><br><span class="line">[ceph_node2][DEBUG ]     &#125;</span><br><span class="line">[ceph_node2][DEBUG ]   ], </span><br><span class="line">[ceph_node2][DEBUG ]   &quot;feature_map&quot;: &#123;</span><br><span class="line">[ceph_node2][DEBUG ]     &quot;mon&quot;: [</span><br><span class="line">[ceph_node2][DEBUG ]       &#123;</span><br><span class="line">[ceph_node2][DEBUG ]         &quot;features&quot;: &quot;0x3ffddff8ffecffff&quot;, </span><br><span class="line">[ceph_node2][DEBUG ]         &quot;num&quot;: 1, </span><br><span class="line">[ceph_node2][DEBUG ]         &quot;release&quot;: &quot;luminous&quot;</span><br><span class="line">[ceph_node2][DEBUG ]       &#125;</span><br><span class="line">[ceph_node2][DEBUG ]     ]</span><br><span class="line">[ceph_node2][DEBUG ]   &#125;, </span><br><span class="line">[ceph_node2][DEBUG ]   &quot;features&quot;: &#123;</span><br><span class="line">[ceph_node2][DEBUG ]     &quot;quorum_con&quot;: &quot;4611087854035861503&quot;, </span><br><span class="line">[ceph_node2][DEBUG ]     &quot;quorum_mon&quot;: [</span><br><span class="line">[ceph_node2][DEBUG ]       &quot;kraken&quot;, </span><br><span class="line">[ceph_node2][DEBUG ]       &quot;luminous&quot;, </span><br><span class="line">[ceph_node2][DEBUG ]       &quot;mimic&quot;, </span><br><span class="line">[ceph_node2][DEBUG ]       &quot;osdmap-prune&quot;, </span><br><span class="line">[ceph_node2][DEBUG ]       &quot;nautilus&quot;</span><br><span class="line">[ceph_node2][DEBUG ]     ], </span><br><span class="line">[ceph_node2][DEBUG ]     &quot;required_con&quot;: &quot;2449958747315912708&quot;, </span><br><span class="line">[ceph_node2][DEBUG ]     &quot;required_mon&quot;: [</span><br><span class="line">[ceph_node2][DEBUG ]       &quot;kraken&quot;, </span><br><span class="line">[ceph_node2][DEBUG ]       &quot;luminous&quot;, </span><br><span class="line">[ceph_node2][DEBUG ]       &quot;mimic&quot;, </span><br><span class="line">[ceph_node2][DEBUG ]       &quot;osdmap-prune&quot;, </span><br><span class="line">[ceph_node2][DEBUG ]       &quot;nautilus&quot;</span><br><span class="line">[ceph_node2][DEBUG ]     ]</span><br><span class="line">[ceph_node2][DEBUG ]   &#125;, </span><br><span class="line">[ceph_node2][DEBUG ]   &quot;monmap&quot;: &#123;</span><br><span class="line">[ceph_node2][DEBUG ]     &quot;created&quot;: &quot;2022-02-18 13:54:10.100119&quot;, </span><br><span class="line">[ceph_node2][DEBUG ]     &quot;epoch&quot;: 1, </span><br><span class="line">[ceph_node2][DEBUG ]     &quot;features&quot;: &#123;</span><br><span class="line">[ceph_node2][DEBUG ]       &quot;optional&quot;: [], </span><br><span class="line">[ceph_node2][DEBUG ]       &quot;persistent&quot;: [</span><br><span class="line">[ceph_node2][DEBUG ]         &quot;kraken&quot;, </span><br><span class="line">[ceph_node2][DEBUG ]         &quot;luminous&quot;, </span><br><span class="line">[ceph_node2][DEBUG ]         &quot;mimic&quot;, </span><br><span class="line">[ceph_node2][DEBUG ]         &quot;osdmap-prune&quot;, </span><br><span class="line">[ceph_node2][DEBUG ]         &quot;nautilus&quot;</span><br><span class="line">[ceph_node2][DEBUG ]       ]</span><br><span class="line">[ceph_node2][DEBUG ]     &#125;, </span><br><span class="line">[ceph_node2][DEBUG ]     &quot;fsid&quot;: &quot;f7be9981-1e57-4165-989e-3c7206331a20&quot;, </span><br><span class="line">[ceph_node2][DEBUG ]     &quot;min_mon_release&quot;: 14, </span><br><span class="line">[ceph_node2][DEBUG ]     &quot;min_mon_release_name&quot;: &quot;nautilus&quot;, </span><br><span class="line">[ceph_node2][DEBUG ]     &quot;modified&quot;: &quot;2022-02-18 13:54:10.100119&quot;, </span><br><span class="line">[ceph_node2][DEBUG ]     &quot;mons&quot;: [</span><br><span class="line">[ceph_node2][DEBUG ]       &#123;</span><br><span class="line">[ceph_node2][DEBUG ]         &quot;addr&quot;: &quot;192.168.111.11:6789/0&quot;, </span><br><span class="line">[ceph_node2][DEBUG ]         &quot;name&quot;: &quot;ceph_node1&quot;, </span><br><span class="line">[ceph_node2][DEBUG ]         &quot;public_addr&quot;: &quot;192.168.111.11:6789/0&quot;, </span><br><span class="line">[ceph_node2][DEBUG ]         &quot;public_addrs&quot;: &#123;</span><br><span class="line">[ceph_node2][DEBUG ]           &quot;addrvec&quot;: [</span><br><span class="line">[ceph_node2][DEBUG ]             &#123;</span><br><span class="line">[ceph_node2][DEBUG ]               &quot;addr&quot;: &quot;192.168.111.11:3300&quot;, </span><br><span class="line">[ceph_node2][DEBUG ]               &quot;nonce&quot;: 0, </span><br><span class="line">[ceph_node2][DEBUG ]               &quot;type&quot;: &quot;v2&quot;</span><br><span class="line">[ceph_node2][DEBUG ]             &#125;, </span><br><span class="line">[ceph_node2][DEBUG ]             &#123;</span><br><span class="line">[ceph_node2][DEBUG ]               &quot;addr&quot;: &quot;192.168.111.11:6789&quot;, </span><br><span class="line">[ceph_node2][DEBUG ]               &quot;nonce&quot;: 0, </span><br><span class="line">[ceph_node2][DEBUG ]               &quot;type&quot;: &quot;v1&quot;</span><br><span class="line">[ceph_node2][DEBUG ]             &#125;</span><br><span class="line">[ceph_node2][DEBUG ]           ]</span><br><span class="line">[ceph_node2][DEBUG ]         &#125;, </span><br><span class="line">[ceph_node2][DEBUG ]         &quot;rank&quot;: 0</span><br><span class="line">[ceph_node2][DEBUG ]       &#125;, </span><br><span class="line">[ceph_node2][DEBUG ]       &#123;</span><br><span class="line">[ceph_node2][DEBUG ]         &quot;addr&quot;: &quot;192.168.111.12:6789/0&quot;, </span><br><span class="line">[ceph_node2][DEBUG ]         &quot;name&quot;: &quot;ceph_node2&quot;, </span><br><span class="line">[ceph_node2][DEBUG ]         &quot;public_addr&quot;: &quot;192.168.111.12:6789/0&quot;, </span><br><span class="line">[ceph_node2][DEBUG ]         &quot;public_addrs&quot;: &#123;</span><br><span class="line">[ceph_node2][DEBUG ]           &quot;addrvec&quot;: [</span><br><span class="line">[ceph_node2][DEBUG ]             &#123;</span><br><span class="line">[ceph_node2][DEBUG ]               &quot;addr&quot;: &quot;192.168.111.12:3300&quot;, </span><br><span class="line">[ceph_node2][DEBUG ]               &quot;nonce&quot;: 0, </span><br><span class="line">[ceph_node2][DEBUG ]               &quot;type&quot;: &quot;v2&quot;</span><br><span class="line">[ceph_node2][DEBUG ]             &#125;, </span><br><span class="line">[ceph_node2][DEBUG ]             &#123;</span><br><span class="line">[ceph_node2][DEBUG ]               &quot;addr&quot;: &quot;192.168.111.12:6789&quot;, </span><br><span class="line">[ceph_node2][DEBUG ]               &quot;nonce&quot;: 0, </span><br><span class="line">[ceph_node2][DEBUG ]               &quot;type&quot;: &quot;v1&quot;</span><br><span class="line">[ceph_node2][DEBUG ]             &#125;</span><br><span class="line">[ceph_node2][DEBUG ]           ]</span><br><span class="line">[ceph_node2][DEBUG ]         &#125;, </span><br><span class="line">[ceph_node2][DEBUG ]         &quot;rank&quot;: 1</span><br><span class="line">[ceph_node2][DEBUG ]       &#125;, </span><br><span class="line">[ceph_node2][DEBUG ]       &#123;</span><br><span class="line">[ceph_node2][DEBUG ]         &quot;addr&quot;: &quot;192.168.111.13:6789/0&quot;, </span><br><span class="line">[ceph_node2][DEBUG ]         &quot;name&quot;: &quot;ceph_node3&quot;, </span><br><span class="line">[ceph_node2][DEBUG ]         &quot;public_addr&quot;: &quot;192.168.111.13:6789/0&quot;, </span><br><span class="line">[ceph_node2][DEBUG ]         &quot;public_addrs&quot;: &#123;</span><br><span class="line">[ceph_node2][DEBUG ]           &quot;addrvec&quot;: [</span><br><span class="line">[ceph_node2][DEBUG ]             &#123;</span><br><span class="line">[ceph_node2][DEBUG ]               &quot;addr&quot;: &quot;192.168.111.13:3300&quot;, </span><br><span class="line">[ceph_node2][DEBUG ]               &quot;nonce&quot;: 0, </span><br><span class="line">[ceph_node2][DEBUG ]               &quot;type&quot;: &quot;v2&quot;</span><br><span class="line">[ceph_node2][DEBUG ]             &#125;, </span><br><span class="line">[ceph_node2][DEBUG ]             &#123;</span><br><span class="line">[ceph_node2][DEBUG ]               &quot;addr&quot;: &quot;192.168.111.13:6789&quot;, </span><br><span class="line">[ceph_node2][DEBUG ]               &quot;nonce&quot;: 0, </span><br><span class="line">[ceph_node2][DEBUG ]               &quot;type&quot;: &quot;v1&quot;</span><br><span class="line">[ceph_node2][DEBUG ]             &#125;</span><br><span class="line">[ceph_node2][DEBUG ]           ]</span><br><span class="line">[ceph_node2][DEBUG ]         &#125;, </span><br><span class="line">[ceph_node2][DEBUG ]         &quot;rank&quot;: 2</span><br><span class="line">[ceph_node2][DEBUG ]       &#125;</span><br><span class="line">[ceph_node2][DEBUG ]     ]</span><br><span class="line">[ceph_node2][DEBUG ]   &#125;, </span><br><span class="line">[ceph_node2][DEBUG ]   &quot;name&quot;: &quot;ceph_node2&quot;, </span><br><span class="line">[ceph_node2][DEBUG ]   &quot;outside_quorum&quot;: [], </span><br><span class="line">[ceph_node2][DEBUG ]   &quot;quorum&quot;: [</span><br><span class="line">[ceph_node2][DEBUG ]     0, </span><br><span class="line">[ceph_node2][DEBUG ]     1, </span><br><span class="line">[ceph_node2][DEBUG ]     2</span><br><span class="line">[ceph_node2][DEBUG ]   ], </span><br><span class="line">[ceph_node2][DEBUG ]   &quot;quorum_age&quot;: 37, </span><br><span class="line">[ceph_node2][DEBUG ]   &quot;rank&quot;: 1, </span><br><span class="line">[ceph_node2][DEBUG ]   &quot;state&quot;: &quot;peon&quot;, </span><br><span class="line">[ceph_node2][DEBUG ]   &quot;sync_provider&quot;: []</span><br><span class="line">[ceph_node2][DEBUG ] &#125;</span><br><span class="line">[ceph_node2][DEBUG ] ********************************************************************************</span><br><span class="line">[ceph_node2][INFO  ] monitor: mon.ceph_node2 is running</span><br><span class="line">[ceph_node2][INFO  ] Running command: ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.ceph_node2.asok mon_status</span><br><span class="line">[ceph_deploy.mon][DEBUG ] detecting platform for host ceph_node3 ...</span><br><span class="line">[ceph_node3][DEBUG ] connected to host: ceph_node3 </span><br><span class="line">[ceph_node3][DEBUG ] detect platform information from remote host</span><br><span class="line">[ceph_node3][DEBUG ] detect machine type</span><br><span class="line">[ceph_node3][DEBUG ] find the location of an executable</span><br><span class="line">[ceph_deploy.mon][INFO  ] distro info: CentOS Linux 7.9.2009 Core</span><br><span class="line">[ceph_node3][DEBUG ] determining if provided host has same hostname in remote</span><br><span class="line">[ceph_node3][DEBUG ] get remote short hostname</span><br><span class="line">[ceph_node3][DEBUG ] deploying mon to ceph_node3</span><br><span class="line">[ceph_node3][DEBUG ] get remote short hostname</span><br><span class="line">[ceph_node3][DEBUG ] remote hostname: ceph_node3</span><br><span class="line">[ceph_node3][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf</span><br><span class="line">[ceph_node3][DEBUG ] create the mon path if it does not exist</span><br><span class="line">[ceph_node3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-ceph_node3/done</span><br><span class="line">[ceph_node3][DEBUG ] create a done file to avoid re-doing the mon deployment</span><br><span class="line">[ceph_node3][DEBUG ] create the init path if it does not exist</span><br><span class="line">[ceph_node3][INFO  ] Running command: systemctl enable ceph.target</span><br><span class="line">[ceph_node3][INFO  ] Running command: systemctl enable ceph-mon@ceph_node3</span><br><span class="line">[ceph_node3][INFO  ] Running command: systemctl start ceph-mon@ceph_node3</span><br><span class="line">[ceph_node3][INFO  ] Running command: ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.ceph_node3.asok mon_status</span><br><span class="line">[ceph_node3][DEBUG ] ********************************************************************************</span><br><span class="line">[ceph_node3][DEBUG ] status for monitor: mon.ceph_node3</span><br><span class="line">[ceph_node3][DEBUG ] &#123;</span><br><span class="line">[ceph_node3][DEBUG ]   &quot;election_epoch&quot;: 6, </span><br><span class="line">[ceph_node3][DEBUG ]   &quot;extra_probe_peers&quot;: [</span><br><span class="line">[ceph_node3][DEBUG ]     &#123;</span><br><span class="line">[ceph_node3][DEBUG ]       &quot;addrvec&quot;: [</span><br><span class="line">[ceph_node3][DEBUG ]         &#123;</span><br><span class="line">[ceph_node3][DEBUG ]           &quot;addr&quot;: &quot;192.168.111.11:3300&quot;, </span><br><span class="line">[ceph_node3][DEBUG ]           &quot;nonce&quot;: 0, </span><br><span class="line">[ceph_node3][DEBUG ]           &quot;type&quot;: &quot;v2&quot;</span><br><span class="line">[ceph_node3][DEBUG ]         &#125;, </span><br><span class="line">[ceph_node3][DEBUG ]         &#123;</span><br><span class="line">[ceph_node3][DEBUG ]           &quot;addr&quot;: &quot;192.168.111.11:6789&quot;, </span><br><span class="line">[ceph_node3][DEBUG ]           &quot;nonce&quot;: 0, </span><br><span class="line">[ceph_node3][DEBUG ]           &quot;type&quot;: &quot;v1&quot;</span><br><span class="line">[ceph_node3][DEBUG ]         &#125;</span><br><span class="line">[ceph_node3][DEBUG ]       ]</span><br><span class="line">[ceph_node3][DEBUG ]     &#125;, </span><br><span class="line">[ceph_node3][DEBUG ]     &#123;</span><br><span class="line">[ceph_node3][DEBUG ]       &quot;addrvec&quot;: [</span><br><span class="line">[ceph_node3][DEBUG ]         &#123;</span><br><span class="line">[ceph_node3][DEBUG ]           &quot;addr&quot;: &quot;192.168.111.12:3300&quot;, </span><br><span class="line">[ceph_node3][DEBUG ]           &quot;nonce&quot;: 0, </span><br><span class="line">[ceph_node3][DEBUG ]           &quot;type&quot;: &quot;v2&quot;</span><br><span class="line">[ceph_node3][DEBUG ]         &#125;, </span><br><span class="line">[ceph_node3][DEBUG ]         &#123;</span><br><span class="line">[ceph_node3][DEBUG ]           &quot;addr&quot;: &quot;192.168.111.12:6789&quot;, </span><br><span class="line">[ceph_node3][DEBUG ]           &quot;nonce&quot;: 0, </span><br><span class="line">[ceph_node3][DEBUG ]           &quot;type&quot;: &quot;v1&quot;</span><br><span class="line">[ceph_node3][DEBUG ]         &#125;</span><br><span class="line">[ceph_node3][DEBUG ]       ]</span><br><span class="line">[ceph_node3][DEBUG ]     &#125;</span><br><span class="line">[ceph_node3][DEBUG ]   ], </span><br><span class="line">[ceph_node3][DEBUG ]   &quot;feature_map&quot;: &#123;</span><br><span class="line">[ceph_node3][DEBUG ]     &quot;mon&quot;: [</span><br><span class="line">[ceph_node3][DEBUG ]       &#123;</span><br><span class="line">[ceph_node3][DEBUG ]         &quot;features&quot;: &quot;0x3ffddff8ffecffff&quot;, </span><br><span class="line">[ceph_node3][DEBUG ]         &quot;num&quot;: 1, </span><br><span class="line">[ceph_node3][DEBUG ]         &quot;release&quot;: &quot;luminous&quot;</span><br><span class="line">[ceph_node3][DEBUG ]       &#125;</span><br><span class="line">[ceph_node3][DEBUG ]     ]</span><br><span class="line">[ceph_node3][DEBUG ]   &#125;, </span><br><span class="line">[ceph_node3][DEBUG ]   &quot;features&quot;: &#123;</span><br><span class="line">[ceph_node3][DEBUG ]     &quot;quorum_con&quot;: &quot;4611087854035861503&quot;, </span><br><span class="line">[ceph_node3][DEBUG ]     &quot;quorum_mon&quot;: [</span><br><span class="line">[ceph_node3][DEBUG ]       &quot;kraken&quot;, </span><br><span class="line">[ceph_node3][DEBUG ]       &quot;luminous&quot;, </span><br><span class="line">[ceph_node3][DEBUG ]       &quot;mimic&quot;, </span><br><span class="line">[ceph_node3][DEBUG ]       &quot;osdmap-prune&quot;, </span><br><span class="line">[ceph_node3][DEBUG ]       &quot;nautilus&quot;</span><br><span class="line">[ceph_node3][DEBUG ]     ], </span><br><span class="line">[ceph_node3][DEBUG ]     &quot;required_con&quot;: &quot;2449958747315912708&quot;, </span><br><span class="line">[ceph_node3][DEBUG ]     &quot;required_mon&quot;: [</span><br><span class="line">[ceph_node3][DEBUG ]       &quot;kraken&quot;, </span><br><span class="line">[ceph_node3][DEBUG ]       &quot;luminous&quot;, </span><br><span class="line">[ceph_node3][DEBUG ]       &quot;mimic&quot;, </span><br><span class="line">[ceph_node3][DEBUG ]       &quot;osdmap-prune&quot;, </span><br><span class="line">[ceph_node3][DEBUG ]       &quot;nautilus&quot;</span><br><span class="line">[ceph_node3][DEBUG ]     ]</span><br><span class="line">[ceph_node3][DEBUG ]   &#125;, </span><br><span class="line">[ceph_node3][DEBUG ]   &quot;monmap&quot;: &#123;</span><br><span class="line">[ceph_node3][DEBUG ]     &quot;created&quot;: &quot;2022-02-18 13:54:10.100119&quot;, </span><br><span class="line">[ceph_node3][DEBUG ]     &quot;epoch&quot;: 1, </span><br><span class="line">[ceph_node3][DEBUG ]     &quot;features&quot;: &#123;</span><br><span class="line">[ceph_node3][DEBUG ]       &quot;optional&quot;: [], </span><br><span class="line">[ceph_node3][DEBUG ]       &quot;persistent&quot;: [</span><br><span class="line">[ceph_node3][DEBUG ]         &quot;kraken&quot;, </span><br><span class="line">[ceph_node3][DEBUG ]         &quot;luminous&quot;, </span><br><span class="line">[ceph_node3][DEBUG ]         &quot;mimic&quot;, </span><br><span class="line">[ceph_node3][DEBUG ]         &quot;osdmap-prune&quot;, </span><br><span class="line">[ceph_node3][DEBUG ]         &quot;nautilus&quot;</span><br><span class="line">[ceph_node3][DEBUG ]       ]</span><br><span class="line">[ceph_node3][DEBUG ]     &#125;, </span><br><span class="line">[ceph_node3][DEBUG ]     &quot;fsid&quot;: &quot;f7be9981-1e57-4165-989e-3c7206331a20&quot;, </span><br><span class="line">[ceph_node3][DEBUG ]     &quot;min_mon_release&quot;: 14, </span><br><span class="line">[ceph_node3][DEBUG ]     &quot;min_mon_release_name&quot;: &quot;nautilus&quot;, </span><br><span class="line">[ceph_node3][DEBUG ]     &quot;modified&quot;: &quot;2022-02-18 13:54:10.100119&quot;, </span><br><span class="line">[ceph_node3][DEBUG ]     &quot;mons&quot;: [</span><br><span class="line">[ceph_node3][DEBUG ]       &#123;</span><br><span class="line">[ceph_node3][DEBUG ]         &quot;addr&quot;: &quot;192.168.111.11:6789/0&quot;, </span><br><span class="line">[ceph_node3][DEBUG ]         &quot;name&quot;: &quot;ceph_node1&quot;, </span><br><span class="line">[ceph_node3][DEBUG ]         &quot;public_addr&quot;: &quot;192.168.111.11:6789/0&quot;, </span><br><span class="line">[ceph_node3][DEBUG ]         &quot;public_addrs&quot;: &#123;</span><br><span class="line">[ceph_node3][DEBUG ]           &quot;addrvec&quot;: [</span><br><span class="line">[ceph_node3][DEBUG ]             &#123;</span><br><span class="line">[ceph_node3][DEBUG ]               &quot;addr&quot;: &quot;192.168.111.11:3300&quot;, </span><br><span class="line">[ceph_node3][DEBUG ]               &quot;nonce&quot;: 0, </span><br><span class="line">[ceph_node3][DEBUG ]               &quot;type&quot;: &quot;v2&quot;</span><br><span class="line">[ceph_node3][DEBUG ]             &#125;, </span><br><span class="line">[ceph_node3][DEBUG ]             &#123;</span><br><span class="line">[ceph_node3][DEBUG ]               &quot;addr&quot;: &quot;192.168.111.11:6789&quot;, </span><br><span class="line">[ceph_node3][DEBUG ]               &quot;nonce&quot;: 0, </span><br><span class="line">[ceph_node3][DEBUG ]               &quot;type&quot;: &quot;v1&quot;</span><br><span class="line">[ceph_node3][DEBUG ]             &#125;</span><br><span class="line">[ceph_node3][DEBUG ]           ]</span><br><span class="line">[ceph_node3][DEBUG ]         &#125;, </span><br><span class="line">[ceph_node3][DEBUG ]         &quot;rank&quot;: 0</span><br><span class="line">[ceph_node3][DEBUG ]       &#125;, </span><br><span class="line">[ceph_node3][DEBUG ]       &#123;</span><br><span class="line">[ceph_node3][DEBUG ]         &quot;addr&quot;: &quot;192.168.111.12:6789/0&quot;, </span><br><span class="line">[ceph_node3][DEBUG ]         &quot;name&quot;: &quot;ceph_node2&quot;, </span><br><span class="line">[ceph_node3][DEBUG ]         &quot;public_addr&quot;: &quot;192.168.111.12:6789/0&quot;, </span><br><span class="line">[ceph_node3][DEBUG ]         &quot;public_addrs&quot;: &#123;</span><br><span class="line">[ceph_node3][DEBUG ]           &quot;addrvec&quot;: [</span><br><span class="line">[ceph_node3][DEBUG ]             &#123;</span><br><span class="line">[ceph_node3][DEBUG ]               &quot;addr&quot;: &quot;192.168.111.12:3300&quot;, </span><br><span class="line">[ceph_node3][DEBUG ]               &quot;nonce&quot;: 0, </span><br><span class="line">[ceph_node3][DEBUG ]               &quot;type&quot;: &quot;v2&quot;</span><br><span class="line">[ceph_node3][DEBUG ]             &#125;, </span><br><span class="line">[ceph_node3][DEBUG ]             &#123;</span><br><span class="line">[ceph_node3][DEBUG ]               &quot;addr&quot;: &quot;192.168.111.12:6789&quot;, </span><br><span class="line">[ceph_node3][DEBUG ]               &quot;nonce&quot;: 0, </span><br><span class="line">[ceph_node3][DEBUG ]               &quot;type&quot;: &quot;v1&quot;</span><br><span class="line">[ceph_node3][DEBUG ]             &#125;</span><br><span class="line">[ceph_node3][DEBUG ]           ]</span><br><span class="line">[ceph_node3][DEBUG ]         &#125;, </span><br><span class="line">[ceph_node3][DEBUG ]         &quot;rank&quot;: 1</span><br><span class="line">[ceph_node3][DEBUG ]       &#125;, </span><br><span class="line">[ceph_node3][DEBUG ]       &#123;</span><br><span class="line">[ceph_node3][DEBUG ]         &quot;addr&quot;: &quot;192.168.111.13:6789/0&quot;, </span><br><span class="line">[ceph_node3][DEBUG ]         &quot;name&quot;: &quot;ceph_node3&quot;, </span><br><span class="line">[ceph_node3][DEBUG ]         &quot;public_addr&quot;: &quot;192.168.111.13:6789/0&quot;, </span><br><span class="line">[ceph_node3][DEBUG ]         &quot;public_addrs&quot;: &#123;</span><br><span class="line">[ceph_node3][DEBUG ]           &quot;addrvec&quot;: [</span><br><span class="line">[ceph_node3][DEBUG ]             &#123;</span><br><span class="line">[ceph_node3][DEBUG ]               &quot;addr&quot;: &quot;192.168.111.13:3300&quot;, </span><br><span class="line">[ceph_node3][DEBUG ]               &quot;nonce&quot;: 0, </span><br><span class="line">[ceph_node3][DEBUG ]               &quot;type&quot;: &quot;v2&quot;</span><br><span class="line">[ceph_node3][DEBUG ]             &#125;, </span><br><span class="line">[ceph_node3][DEBUG ]             &#123;</span><br><span class="line">[ceph_node3][DEBUG ]               &quot;addr&quot;: &quot;192.168.111.13:6789&quot;, </span><br><span class="line">[ceph_node3][DEBUG ]               &quot;nonce&quot;: 0, </span><br><span class="line">[ceph_node3][DEBUG ]               &quot;type&quot;: &quot;v1&quot;</span><br><span class="line">[ceph_node3][DEBUG ]             &#125;</span><br><span class="line">[ceph_node3][DEBUG ]           ]</span><br><span class="line">[ceph_node3][DEBUG ]         &#125;, </span><br><span class="line">[ceph_node3][DEBUG ]         &quot;rank&quot;: 2</span><br><span class="line">[ceph_node3][DEBUG ]       &#125;</span><br><span class="line">[ceph_node3][DEBUG ]     ]</span><br><span class="line">[ceph_node3][DEBUG ]   &#125;, </span><br><span class="line">[ceph_node3][DEBUG ]   &quot;name&quot;: &quot;ceph_node3&quot;, </span><br><span class="line">[ceph_node3][DEBUG ]   &quot;outside_quorum&quot;: [], </span><br><span class="line">[ceph_node3][DEBUG ]   &quot;quorum&quot;: [</span><br><span class="line">[ceph_node3][DEBUG ]     0, </span><br><span class="line">[ceph_node3][DEBUG ]     1, </span><br><span class="line">[ceph_node3][DEBUG ]     2</span><br><span class="line">[ceph_node3][DEBUG ]   ], </span><br><span class="line">[ceph_node3][DEBUG ]   &quot;quorum_age&quot;: 41, </span><br><span class="line">[ceph_node3][DEBUG ]   &quot;rank&quot;: 2, </span><br><span class="line">[ceph_node3][DEBUG ]   &quot;state&quot;: &quot;peon&quot;, </span><br><span class="line">[ceph_node3][DEBUG ]   &quot;sync_provider&quot;: []</span><br><span class="line">[ceph_node3][DEBUG ] &#125;</span><br><span class="line">[ceph_node3][DEBUG ] ********************************************************************************</span><br><span class="line">[ceph_node3][INFO  ] monitor: mon.ceph_node3 is running</span><br><span class="line">[ceph_node3][INFO  ] Running command: ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.ceph_node3.asok mon_status</span><br><span class="line">[ceph_deploy.mon][INFO  ] processing monitor mon.ceph_node1</span><br><span class="line">[ceph_node1][DEBUG ] connected to host: ceph_node1 </span><br><span class="line">[ceph_node1][DEBUG ] detect platform information from remote host</span><br><span class="line">[ceph_node1][DEBUG ] detect machine type</span><br><span class="line">[ceph_node1][DEBUG ] find the location of an executable</span><br><span class="line">[ceph_node1][INFO  ] Running command: ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.ceph_node1.asok mon_status</span><br><span class="line">[ceph_deploy.mon][INFO  ] mon.ceph_node1 monitor has reached quorum!</span><br><span class="line">[ceph_deploy.mon][INFO  ] processing monitor mon.ceph_node2</span><br><span class="line">[ceph_node2][DEBUG ] connected to host: ceph_node2 </span><br><span class="line">[ceph_node2][DEBUG ] detect platform information from remote host</span><br><span class="line">[ceph_node2][DEBUG ] detect machine type</span><br><span class="line">[ceph_node2][DEBUG ] find the location of an executable</span><br><span class="line">[ceph_node2][INFO  ] Running command: ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.ceph_node2.asok mon_status</span><br><span class="line">[ceph_deploy.mon][INFO  ] mon.ceph_node2 monitor has reached quorum!</span><br><span class="line">[ceph_deploy.mon][INFO  ] processing monitor mon.ceph_node3</span><br><span class="line">[ceph_node3][DEBUG ] connected to host: ceph_node3 </span><br><span class="line">[ceph_node3][DEBUG ] detect platform information from remote host</span><br><span class="line">[ceph_node3][DEBUG ] detect machine type</span><br><span class="line">[ceph_node3][DEBUG ] find the location of an executable</span><br><span class="line">[ceph_node3][INFO  ] Running command: ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.ceph_node3.asok mon_status</span><br><span class="line">[ceph_deploy.mon][INFO  ] mon.ceph_node3 monitor has reached quorum!</span><br><span class="line">[ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum</span><br><span class="line">[ceph_deploy.mon][INFO  ] Running gatherkeys...</span><br><span class="line">[ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpRev3cc</span><br><span class="line">[ceph_node1][DEBUG ] connected to host: ceph_node1 </span><br><span class="line">[ceph_node1][DEBUG ] detect platform information from remote host</span><br><span class="line">[ceph_node1][DEBUG ] detect machine type</span><br><span class="line">[ceph_node1][DEBUG ] get remote short hostname</span><br><span class="line">[ceph_node1][DEBUG ] fetch remote file</span><br><span class="line">[ceph_node1][INFO  ] Running command: /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.ceph_node1.asok mon_status</span><br><span class="line">[ceph_node1][INFO  ] Running command: /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-ceph_node1/keyring auth get client.admin</span><br><span class="line">[ceph_node1][INFO  ] Running command: /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-ceph_node1/keyring auth get client.bootstrap-mds</span><br><span class="line">[ceph_node1][INFO  ] Running command: /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-ceph_node1/keyring auth get client.bootstrap-mgr</span><br><span class="line">[ceph_node1][INFO  ] Running command: /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-ceph_node1/keyring auth get client.bootstrap-osd</span><br><span class="line">[ceph_node1][INFO  ] Running command: /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-ceph_node1/keyring auth get client.bootstrap-rgw</span><br><span class="line">[ceph_deploy.gatherkeys][INFO  ] Storing ceph.client.admin.keyring</span><br><span class="line">[ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mds.keyring</span><br><span class="line">[ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mgr.keyring</span><br><span class="line">[ceph_deploy.gatherkeys][INFO  ] keyring &#x27;ceph.mon.keyring&#x27; already exists</span><br><span class="line">[ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-osd.keyring</span><br><span class="line">[ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-rgw.keyring</span><br><span class="line">[ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpRev3cc</span><br></pre></td></tr></table></figure><h2 id="生成ceph-admin秘钥"><a href="#生成ceph-admin秘钥" class="headerlink" title="生成ceph admin秘钥"></a>生成ceph admin秘钥</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph_node1 cluster]# ceph-deploy admin ceph_node1 ceph_node2 ceph_node3 </span><br><span class="line"></span><br><span class="line">[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf</span><br><span class="line">[ceph_deploy.cli][INFO  ] Invoked (2.0.1): /usr/bin/ceph-deploy admin ceph_node1 ceph_node2 ceph_node3</span><br><span class="line">[ceph_deploy.cli][INFO  ] ceph-deploy options:</span><br><span class="line">[ceph_deploy.cli][INFO  ]  username                      : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  verbose                       : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  overwrite_conf                : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  quiet                         : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7f8101aeb368&gt;</span><br><span class="line">[ceph_deploy.cli][INFO  ]  cluster                       : ceph</span><br><span class="line">[ceph_deploy.cli][INFO  ]  client                        : [&#x27;ceph_node1&#x27;, &#x27;ceph_node2&#x27;, &#x27;ceph_node3&#x27;]</span><br><span class="line">[ceph_deploy.cli][INFO  ]  func                          : &lt;function admin at 0x7f81025fc230&gt;</span><br><span class="line">[ceph_deploy.cli][INFO  ]  ceph_conf                     : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  default_release               : False</span><br><span class="line">[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph_node1</span><br><span class="line">[ceph_node1][DEBUG ] connected to host: ceph_node1 </span><br><span class="line">[ceph_node1][DEBUG ] detect platform information from remote host</span><br><span class="line">[ceph_node1][DEBUG ] detect machine type</span><br><span class="line">[ceph_node1][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf</span><br><span class="line">[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph_node2</span><br><span class="line">[ceph_node2][DEBUG ] connected to host: ceph_node2 </span><br><span class="line">[ceph_node2][DEBUG ] detect platform information from remote host</span><br><span class="line">[ceph_node2][DEBUG ] detect machine type</span><br><span class="line">[ceph_node2][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf</span><br><span class="line">[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph_node3</span><br><span class="line">[ceph_node3][DEBUG ] connected to host: ceph_node3 </span><br><span class="line">[ceph_node3][DEBUG ] detect platform information from remote host</span><br><span class="line">[ceph_node3][DEBUG ] detect machine type</span><br><span class="line">[ceph_node3][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf</span><br></pre></td></tr></table></figure><h2 id="部署MGR，提供web界面管理ceph（可选安装）"><a href="#部署MGR，提供web界面管理ceph（可选安装）" class="headerlink" title="部署MGR，提供web界面管理ceph（可选安装）"></a>部署MGR，提供web界面管理ceph（可选安装）</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph_node1 cluster]# ceph-deploy mgr create ceph_node1 ceph_node2 ceph_node3 </span><br><span class="line"></span><br><span class="line">[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf</span><br><span class="line">[ceph_deploy.cli][INFO  ] Invoked (2.0.1): /usr/bin/ceph-deploy mgr create ceph_node1 ceph_node2 ceph_node3</span><br><span class="line">[ceph_deploy.cli][INFO  ] ceph-deploy options:</span><br><span class="line">[ceph_deploy.cli][INFO  ]  username                      : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  verbose                       : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  mgr                           : [(&#x27;ceph_node1&#x27;, &#x27;ceph_node1&#x27;), (&#x27;ceph_node2&#x27;, &#x27;ceph_node2&#x27;), (&#x27;ceph_node3&#x27;, &#x27;ceph_node3&#x27;)]</span><br><span class="line">[ceph_deploy.cli][INFO  ]  overwrite_conf                : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  subcommand                    : create</span><br><span class="line">[ceph_deploy.cli][INFO  ]  quiet                         : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7fb3ff746710&gt;</span><br><span class="line">[ceph_deploy.cli][INFO  ]  cluster                       : ceph</span><br><span class="line">[ceph_deploy.cli][INFO  ]  func                          : &lt;function mgr at 0x7fb3fffac140&gt;</span><br><span class="line">[ceph_deploy.cli][INFO  ]  ceph_conf                     : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  default_release               : False</span><br><span class="line">[ceph_deploy.mgr][DEBUG ] Deploying mgr, cluster ceph hosts ceph_node1:ceph_node1 ceph_node2:ceph_node2 ceph_node3:ceph_node3</span><br><span class="line">[ceph_node1][DEBUG ] connected to host: ceph_node1 </span><br><span class="line">[ceph_node1][DEBUG ] detect platform information from remote host</span><br><span class="line">[ceph_node1][DEBUG ] detect machine type</span><br><span class="line">[ceph_deploy.mgr][INFO  ] Distro info: CentOS Linux 7.9.2009 Core</span><br><span class="line">[ceph_deploy.mgr][DEBUG ] remote host will use systemd</span><br><span class="line">[ceph_deploy.mgr][DEBUG ] deploying mgr bootstrap to ceph_node1</span><br><span class="line">[ceph_node1][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf</span><br><span class="line">[ceph_node1][WARNIN] mgr keyring does not exist yet, creating one</span><br><span class="line">[ceph_node1][DEBUG ] create a keyring file</span><br><span class="line">[ceph_node1][DEBUG ] create path recursively if it doesn&#x27;t exist</span><br><span class="line">[ceph_node1][INFO  ] Running command: ceph --cluster ceph --name client.bootstrap-mgr --keyring /var/lib/ceph/bootstrap-mgr/ceph.keyring auth get-or-create mgr.ceph_node1 mon allow profile mgr osd allow * mds allow * -o /var/lib/ceph/mgr/ceph-ceph_node1/keyring</span><br><span class="line">[ceph_node1][INFO  ] Running command: systemctl enable ceph-mgr@ceph_node1</span><br><span class="line">[ceph_node1][WARNIN] Created symlink from /etc/systemd/system/ceph-mgr.target.wants/ceph-mgr@ceph_node1.service to /usr/lib/systemd/system/ceph-mgr@.service.</span><br><span class="line">[ceph_node1][INFO  ] Running command: systemctl start ceph-mgr@ceph_node1</span><br><span class="line">[ceph_node1][INFO  ] Running command: systemctl enable ceph.target</span><br><span class="line">[ceph_node2][DEBUG ] connected to host: ceph_node2 </span><br><span class="line">[ceph_node2][DEBUG ] detect platform information from remote host</span><br><span class="line">[ceph_node2][DEBUG ] detect machine type</span><br><span class="line">[ceph_deploy.mgr][INFO  ] Distro info: CentOS Linux 7.9.2009 Core</span><br><span class="line">[ceph_deploy.mgr][DEBUG ] remote host will use systemd</span><br><span class="line">[ceph_deploy.mgr][DEBUG ] deploying mgr bootstrap to ceph_node2</span><br><span class="line">[ceph_node2][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf</span><br><span class="line">[ceph_node2][WARNIN] mgr keyring does not exist yet, creating one</span><br><span class="line">[ceph_node2][DEBUG ] create a keyring file</span><br><span class="line">[ceph_node2][DEBUG ] create path recursively if it doesn&#x27;t exist</span><br><span class="line">[ceph_node2][INFO  ] Running command: ceph --cluster ceph --name client.bootstrap-mgr --keyring /var/lib/ceph/bootstrap-mgr/ceph.keyring auth get-or-create mgr.ceph_node2 mon allow profile mgr osd allow * mds allow * -o /var/lib/ceph/mgr/ceph-ceph_node2/keyring</span><br><span class="line">[ceph_node2][INFO  ] Running command: systemctl enable ceph-mgr@ceph_node2</span><br><span class="line">[ceph_node2][WARNIN] Created symlink from /etc/systemd/system/ceph-mgr.target.wants/ceph-mgr@ceph_node2.service to /usr/lib/systemd/system/ceph-mgr@.service.</span><br><span class="line">[ceph_node2][INFO  ] Running command: systemctl start ceph-mgr@ceph_node2</span><br><span class="line">[ceph_node2][INFO  ] Running command: systemctl enable ceph.target</span><br><span class="line">[ceph_node3][DEBUG ] connected to host: ceph_node3 </span><br><span class="line">[ceph_node3][DEBUG ] detect platform information from remote host</span><br><span class="line">[ceph_node3][DEBUG ] detect machine type</span><br><span class="line">[ceph_deploy.mgr][INFO  ] Distro info: CentOS Linux 7.9.2009 Core</span><br><span class="line">[ceph_deploy.mgr][DEBUG ] remote host will use systemd</span><br><span class="line">[ceph_deploy.mgr][DEBUG ] deploying mgr bootstrap to ceph_node3</span><br><span class="line">[ceph_node3][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf</span><br><span class="line">[ceph_node3][WARNIN] mgr keyring does not exist yet, creating one</span><br><span class="line">[ceph_node3][DEBUG ] create a keyring file</span><br><span class="line">[ceph_node3][DEBUG ] create path recursively if it doesn&#x27;t exist</span><br><span class="line">[ceph_node3][INFO  ] Running command: ceph --cluster ceph --name client.bootstrap-mgr --keyring /var/lib/ceph/bootstrap-mgr/ceph.keyring auth get-or-create mgr.ceph_node3 mon allow profile mgr osd allow * mds allow * -o /var/lib/ceph/mgr/ceph-ceph_node3/keyring</span><br><span class="line">[ceph_node3][INFO  ] Running command: systemctl enable ceph-mgr@ceph_node3</span><br><span class="line">[ceph_node3][WARNIN] Created symlink from /etc/systemd/system/ceph-mgr.target.wants/ceph-mgr@ceph_node3.service to /usr/lib/systemd/system/ceph-mgr@.service.</span><br><span class="line">[ceph_node3][INFO  ] Running command: systemctl start ceph-mgr@ceph_node3</span><br><span class="line">[ceph_node3][INFO  ] Running command: systemctl enable ceph.target</span><br></pre></td></tr></table></figure><h2 id="部署rgw"><a href="#部署rgw" class="headerlink" title="部署rgw"></a>部署rgw</h2><p><code>yum -y install ceph-radosgw</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph_node1 cluster]# ceph-deploy rgw create ceph_node1</span><br><span class="line"></span><br><span class="line">[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf</span><br><span class="line">[ceph_deploy.cli][INFO  ] Invoked (2.0.1): /usr/bin/ceph-deploy rgw create ceph_node1</span><br><span class="line">[ceph_deploy.cli][INFO  ] ceph-deploy options:</span><br><span class="line">[ceph_deploy.cli][INFO  ]  username                      : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  verbose                       : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  rgw                           : [(&#x27;ceph_node1&#x27;, &#x27;rgw.ceph_node1&#x27;)]</span><br><span class="line">[ceph_deploy.cli][INFO  ]  overwrite_conf                : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  subcommand                    : create</span><br><span class="line">[ceph_deploy.cli][INFO  ]  quiet                         : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7f4fe2168b00&gt;</span><br><span class="line">[ceph_deploy.cli][INFO  ]  cluster                       : ceph</span><br><span class="line">[ceph_deploy.cli][INFO  ]  func                          : &lt;function rgw at 0x7f4fe2c35050&gt;</span><br><span class="line">[ceph_deploy.cli][INFO  ]  ceph_conf                     : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  default_release               : False</span><br><span class="line">[ceph_deploy.rgw][DEBUG ] Deploying rgw, cluster ceph hosts ceph_node1:rgw.ceph_node1</span><br><span class="line">[ceph_node1][DEBUG ] connected to host: ceph_node1 </span><br><span class="line">[ceph_node1][DEBUG ] detect platform information from remote host</span><br><span class="line">[ceph_node1][DEBUG ] detect machine type</span><br><span class="line">[ceph_deploy.rgw][INFO  ] Distro info: CentOS Linux 7.9.2009 Core</span><br><span class="line">[ceph_deploy.rgw][DEBUG ] remote host will use systemd</span><br><span class="line">[ceph_deploy.rgw][DEBUG ] deploying rgw bootstrap to ceph_node1</span><br><span class="line">[ceph_node1][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf</span><br><span class="line">[ceph_node1][WARNIN] rgw keyring does not exist yet, creating one</span><br><span class="line">[ceph_node1][DEBUG ] create a keyring file</span><br><span class="line">[ceph_node1][DEBUG ] create path recursively if it doesn&#x27;t exist</span><br><span class="line">[ceph_node1][INFO  ] Running command: ceph --cluster ceph --name client.bootstrap-rgw --keyring /var/lib/ceph/bootstrap-rgw/ceph.keyring auth get-or-create client.rgw.ceph_node1 osd allow rwx mon allow rw -o /var/lib/ceph/radosgw/ceph-rgw.ceph_node1/keyring</span><br><span class="line">[ceph_node1][INFO  ] Running command: systemctl enable ceph-radosgw@rgw.ceph_node1</span><br><span class="line">[ceph_node1][WARNIN] Created symlink from /etc/systemd/system/ceph-radosgw.target.wants/ceph-radosgw@rgw.ceph_node1.service to /usr/lib/systemd/system/ceph-radosgw@.service.</span><br><span class="line">[ceph_node1][INFO  ] Running command: systemctl start ceph-radosgw@rgw.ceph_node1</span><br><span class="line">[ceph_node1][INFO  ] Running command: systemctl enable ceph.target</span><br><span class="line">[ceph_deploy.rgw][INFO  ] The Ceph Object Gateway (RGW) is now running on host ceph_node1 and default port 7480</span><br></pre></td></tr></table></figure><h2 id="部署cephfs（可选）"><a href="#部署cephfs（可选）" class="headerlink" title="部署cephfs（可选）"></a>部署cephfs（可选）</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph_node1 cluster]# ceph-deploy mds create ceph_node1 ceph_node2 ceph_node3</span><br><span class="line"></span><br><span class="line">[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf</span><br><span class="line">[ceph_deploy.cli][INFO  ] Invoked (2.0.1): /usr/bin/ceph-deploy mds create ceph_node1 ceph_node2 ceph_node3</span><br><span class="line">[ceph_deploy.cli][INFO  ] ceph-deploy options:</span><br><span class="line">[ceph_deploy.cli][INFO  ]  username                      : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  verbose                       : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  overwrite_conf                : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  subcommand                    : create</span><br><span class="line">[ceph_deploy.cli][INFO  ]  quiet                         : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7fb889afe2d8&gt;</span><br><span class="line">[ceph_deploy.cli][INFO  ]  cluster                       : ceph</span><br><span class="line">[ceph_deploy.cli][INFO  ]  func                          : &lt;function mds at 0x7fb889b3ced8&gt;</span><br><span class="line">[ceph_deploy.cli][INFO  ]  ceph_conf                     : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  mds                           : [(&#x27;ceph_node1&#x27;, &#x27;ceph_node1&#x27;), (&#x27;ceph_node2&#x27;, &#x27;ceph_node2&#x27;), (&#x27;ceph_node3&#x27;, &#x27;ceph_node3&#x27;)]</span><br><span class="line">[ceph_deploy.cli][INFO  ]  default_release               : False</span><br><span class="line">[ceph_deploy.mds][DEBUG ] Deploying mds, cluster ceph hosts ceph_node1:ceph_node1 ceph_node2:ceph_node2 ceph_node3:ceph_node3</span><br><span class="line">[ceph_node1][DEBUG ] connected to host: ceph_node1 </span><br><span class="line">[ceph_node1][DEBUG ] detect platform information from remote host</span><br><span class="line">[ceph_node1][DEBUG ] detect machine type</span><br><span class="line">[ceph_deploy.mds][INFO  ] Distro info: CentOS Linux 7.9.2009 Core</span><br><span class="line">[ceph_deploy.mds][DEBUG ] remote host will use systemd</span><br><span class="line">[ceph_deploy.mds][DEBUG ] deploying mds bootstrap to ceph_node1</span><br><span class="line">[ceph_node1][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf</span><br><span class="line">[ceph_node1][WARNIN] mds keyring does not exist yet, creating one</span><br><span class="line">[ceph_node1][DEBUG ] create a keyring file</span><br><span class="line">[ceph_node1][DEBUG ] create path if it doesn&#x27;t exist</span><br><span class="line">[ceph_node1][INFO  ] Running command: ceph --cluster ceph --name client.bootstrap-mds --keyring /var/lib/ceph/bootstrap-mds/ceph.keyring auth get-or-create mds.ceph_node1 osd allow rwx mds allow mon allow profile mds -o /var/lib/ceph/mds/ceph-ceph_node1/keyring</span><br><span class="line">[ceph_node1][INFO  ] Running command: systemctl enable ceph-mds@ceph_node1</span><br><span class="line">[ceph_node1][WARNIN] Created symlink from /etc/systemd/system/ceph-mds.target.wants/ceph-mds@ceph_node1.service to /usr/lib/systemd/system/ceph-mds@.service.</span><br><span class="line">[ceph_node1][INFO  ] Running command: systemctl start ceph-mds@ceph_node1</span><br><span class="line">[ceph_node1][INFO  ] Running command: systemctl enable ceph.target</span><br><span class="line">[ceph_node2][DEBUG ] connected to host: ceph_node2 </span><br><span class="line">[ceph_node2][DEBUG ] detect platform information from remote host</span><br><span class="line">[ceph_node2][DEBUG ] detect machine type</span><br><span class="line">[ceph_deploy.mds][INFO  ] Distro info: CentOS Linux 7.9.2009 Core</span><br><span class="line">[ceph_deploy.mds][DEBUG ] remote host will use systemd</span><br><span class="line">[ceph_deploy.mds][DEBUG ] deploying mds bootstrap to ceph_node2</span><br><span class="line">[ceph_node2][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf</span><br><span class="line">[ceph_node2][WARNIN] mds keyring does not exist yet, creating one</span><br><span class="line">[ceph_node2][DEBUG ] create a keyring file</span><br><span class="line">[ceph_node2][DEBUG ] create path if it doesn&#x27;t exist</span><br><span class="line">[ceph_node2][INFO  ] Running command: ceph --cluster ceph --name client.bootstrap-mds --keyring /var/lib/ceph/bootstrap-mds/ceph.keyring auth get-or-create mds.ceph_node2 osd allow rwx mds allow mon allow profile mds -o /var/lib/ceph/mds/ceph-ceph_node2/keyring</span><br><span class="line">[ceph_node2][INFO  ] Running command: systemctl enable ceph-mds@ceph_node2</span><br><span class="line">[ceph_node2][WARNIN] Created symlink from /etc/systemd/system/ceph-mds.target.wants/ceph-mds@ceph_node2.service to /usr/lib/systemd/system/ceph-mds@.service.</span><br><span class="line">[ceph_node2][INFO  ] Running command: systemctl start ceph-mds@ceph_node2</span><br><span class="line">[ceph_node2][INFO  ] Running command: systemctl enable ceph.target</span><br><span class="line">[ceph_node3][DEBUG ] connected to host: ceph_node3 </span><br><span class="line">[ceph_node3][DEBUG ] detect platform information from remote host</span><br><span class="line">[ceph_node3][DEBUG ] detect machine type</span><br><span class="line">[ceph_deploy.mds][INFO  ] Distro info: CentOS Linux 7.9.2009 Core</span><br><span class="line">[ceph_deploy.mds][DEBUG ] remote host will use systemd</span><br><span class="line">[ceph_deploy.mds][DEBUG ] deploying mds bootstrap to ceph_node3</span><br><span class="line">[ceph_node3][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf</span><br><span class="line">[ceph_node3][WARNIN] mds keyring does not exist yet, creating one</span><br><span class="line">[ceph_node3][DEBUG ] create a keyring file</span><br><span class="line">[ceph_node3][DEBUG ] create path if it doesn&#x27;t exist</span><br><span class="line">[ceph_node3][INFO  ] Running command: ceph --cluster ceph --name client.bootstrap-mds --keyring /var/lib/ceph/bootstrap-mds/ceph.keyring auth get-or-create mds.ceph_node3 osd allow rwx mds allow mon allow profile mds -o /var/lib/ceph/mds/ceph-ceph_node3/keyring</span><br><span class="line">[ceph_node3][INFO  ] Running command: systemctl enable ceph-mds@ceph_node3</span><br><span class="line">[ceph_node3][WARNIN] Created symlink from /etc/systemd/system/ceph-mds.target.wants/ceph-mds@ceph_node3.service to /usr/lib/systemd/system/ceph-mds@.service.</span><br><span class="line">[ceph_node3][INFO  ] Running command: systemctl start ceph-mds@ceph_node3</span><br><span class="line">[ceph_node3][INFO  ] Running command: systemctl enable ceph.target</span><br></pre></td></tr></table></figure><h2 id="初始化OSD"><a href="#初始化OSD" class="headerlink" title="初始化OSD"></a>初始化OSD</h2><p>在ceph_node1上一次初始化所有磁盘</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy osd create --data /dev/sdb ceph_node1</span><br><span class="line">ceph-deploy osd create --data /dev/sdc ceph_node1</span><br><span class="line">ceph-deploy osd create --data /dev/sdd ceph_node1</span><br><span class="line">ceph-deploy osd create --data /dev/sdb ceph_node2</span><br><span class="line">ceph-deploy osd create --data /dev/sdc ceph_node2</span><br><span class="line">ceph-deploy osd create --data /dev/sdd ceph_node2</span><br><span class="line">ceph-deploy osd create --data /dev/sdb ceph_node3</span><br><span class="line">ceph-deploy osd create --data /dev/sdc ceph_node3</span><br><span class="line">ceph-deploy osd create --data /dev/sdd ceph_node3</span><br></pre></td></tr></table></figure><h2 id="查看OSD状态"><a href="#查看OSD状态" class="headerlink" title="查看OSD状态"></a>查看OSD状态</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph_node1 cluster]# ceph osd status</span><br><span class="line">+----+------------+-------+-------+--------+---------+--------+---------+-----------+</span><br><span class="line">| id |    host    |  used | avail | wr ops | wr data | rd ops | rd data |   state   |</span><br><span class="line">+----+------------+-------+-------+--------+---------+--------+---------+-----------+</span><br><span class="line">| 0  | ceph_node1 | 1031M | 9204M |    0   |     0   |    0   |     0   | exists,up |</span><br><span class="line">| 1  | ceph_node1 | 1031M | 9204M |    0   |     0   |    0   |     0   | exists,up |</span><br><span class="line">| 2  | ceph_node1 | 1031M | 9204M |    0   |     0   |    0   |     0   | exists,up |</span><br><span class="line">| 3  | ceph_node2 | 1031M | 9204M |    0   |     0   |    0   |     0   | exists,up |</span><br><span class="line">| 4  | ceph_node2 | 1031M | 9204M |    0   |     0   |    0   |     0   | exists,up |</span><br><span class="line">| 5  | ceph_node2 | 1031M | 9204M |    0   |     0   |    0   |     0   | exists,up |</span><br><span class="line">| 6  | ceph_node3 | 1031M | 9204M |    0   |     0   |    0   |     0   | exists,up |</span><br><span class="line">| 7  | ceph_node3 | 1031M | 9204M |    0   |     0   |    0   |     0   | exists,up |</span><br><span class="line">| 8  | ceph_node3 | 1031M | 9204M |    0   |     0   |    0   |     0   | exists,up |</span><br><span class="line">+----+------------+-------+-------+--------+---------+--------+---------+-----------+</span><br></pre></td></tr></table></figure><h2 id="查看ceph状态"><a href="#查看ceph状态" class="headerlink" title="查看ceph状态"></a>查看ceph状态</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph_node1 cluster]# ceph -s</span><br><span class="line">  cluster:</span><br><span class="line">    id:     f7be9981-1e57-4165-989e-3c7206331a20</span><br><span class="line">    health: HEALTH_OK</span><br><span class="line"> </span><br><span class="line">  services:</span><br><span class="line">    mon: 3 daemons, quorum ceph_node1,ceph_node2,ceph_node3 (age 21m)</span><br><span class="line">    mgr: ceph_node1(active, since 16m), standbys: ceph_node2, ceph_node3</span><br><span class="line">    mds:  3 up:standby</span><br><span class="line">    osd: 9 osds: 9 up (since 4m), 9 in (since 4m)</span><br><span class="line">    rgw: 1 daemon active (ceph_node1)</span><br><span class="line"> </span><br><span class="line">  task status:</span><br><span class="line"> </span><br><span class="line">  data:</span><br><span class="line">    pools:   4 pools, 128 pgs</span><br><span class="line">    objects: 187 objects, 1.2 KiB</span><br><span class="line">    usage:   9.1 GiB used, 81 GiB / 90 GiB avail</span><br><span class="line">    pgs:     128 active+clean</span><br></pre></td></tr></table></figure><p>至此，整个ceph的部署就完成了，健康状态也是ok的。</p><h1 id="Ceph操作使用"><a href="#Ceph操作使用" class="headerlink" title="Ceph操作使用"></a>Ceph操作使用</h1><h2 id="创建存储池"><a href="#创建存储池" class="headerlink" title="创建存储池"></a>创建存储池</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph_node1 cluster]# ceph osd pool create mypool1 128</span><br><span class="line">pool &#x27;mypool1&#x27; created</span><br><span class="line"></span><br><span class="line">#查看已创建的存储池信息</span><br><span class="line">[root@ceph_node1 cluster]# ceph osd pool ls </span><br><span class="line">.rgw.root</span><br><span class="line">default.rgw.control</span><br><span class="line">default.rgw.meta</span><br><span class="line">default.rgw.log</span><br><span class="line">mypool1</span><br></pre></td></tr></table></figure><h2 id="查看存储池详细参数"><a href="#查看存储池详细参数" class="headerlink" title="查看存储池详细参数"></a>查看存储池详细参数</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph_node1 cluster]# ceph osd pool ls detail </span><br><span class="line">pool 1 &#x27;.rgw.root&#x27; replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode warn last_change 9 flags hashpspool stripe_width 0 application rgw</span><br><span class="line">pool 2 &#x27;default.rgw.control&#x27; replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode warn last_change 34 flags hashpspool stripe_width 0 application rgw</span><br><span class="line">pool 3 &#x27;default.rgw.meta&#x27; replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode warn last_change 36 flags hashpspool stripe_width 0 application rgw</span><br><span class="line">pool 4 &#x27;default.rgw.log&#x27; replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode warn last_change 38 flags hashpspool stripe_width 0 application rgw</span><br><span class="line">pool 5 &#x27;mypool1&#x27; replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode warn last_change 63 flags hashpspool stripe_width 0</span><br></pre></td></tr></table></figure><h2 id="查看存储池单个参数配置"><a href="#查看存储池单个参数配置" class="headerlink" title="查看存储池单个参数配置"></a>查看存储池单个参数配置</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 查看副本数</span><br><span class="line">[root@ceph_node1 cluster]# ceph osd pool get mypool1 size</span><br><span class="line">size: 3</span><br><span class="line"># 查看pg_num</span><br><span class="line">[root@ceph_node1 cluster]# ceph osd pool get mypool1 pg_num</span><br><span class="line">pg_num: 128</span><br></pre></td></tr></table></figure><h2 id="修改存储池参数"><a href="#修改存储池参数" class="headerlink" title="修改存储池参数"></a>修改存储池参数</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph_node1 cluster]# ceph osd pool set mypool1 pg_num 64</span><br><span class="line">set pool 5 pg_num to 64</span><br></pre></td></tr></table></figure><h2 id="创建EC池，创建EC策略"><a href="#创建EC池，创建EC策略" class="headerlink" title="创建EC池，创建EC策略"></a>创建EC池，创建EC策略</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph_node1 cluster]# ceph osd erasure-code-profile set ec001 k=3 m=2 crush-failure-domain=osd</span><br><span class="line">[root@ceph_node1 cluster]# ceph osd pool create mypool2 100 erasure ec001</span><br><span class="line">pool &#x27;mypool2&#x27; created</span><br><span class="line">[root@ceph_node1 cluster]# ceph osd pool ls detail</span><br><span class="line">pool 1 &#x27;.rgw.root&#x27; replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode warn last_change 9 flags hashpspool stripe_width 0 application rgw</span><br><span class="line">pool 2 &#x27;default.rgw.control&#x27; replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode warn last_change 34 flags hashpspool stripe_width 0 application rgw</span><br><span class="line">pool 3 &#x27;default.rgw.meta&#x27; replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode warn last_change 36 flags hashpspool stripe_width 0 application rgw</span><br><span class="line">pool 4 &#x27;default.rgw.log&#x27; replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode warn last_change 38 flags hashpspool stripe_width 0 application rgw</span><br><span class="line">pool 5 &#x27;mypool1&#x27; replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode warn last_change 302 lfor 0/302/300 flags hashpspool stripe_width 0</span><br><span class="line">pool 6 &#x27;mypool2&#x27; erasure size 5 min_size 4 crush_rule 1 object_hash rjenkins pg_num 100 pgp_num 100 autoscale_mode warn last_change 306 flags hashpspool,creating stripe_width 12288</span><br></pre></td></tr></table></figure><h2 id="rgw使用"><a href="#rgw使用" class="headerlink" title="rgw使用"></a>rgw使用</h2><p>设置mypool2为rgw，使用rados客户端工具测试上传下载</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph_node1 cluster]# ceph osd pool application enable mypool2 rgw</span><br><span class="line">enabled application &#x27;rgw&#x27; on pool &#x27;mypool2&#x27;</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 将/etc/passwd 上到mypool2中，命名为t_pass</span><br><span class="line">[root@ceph_node1 cluster]# rados -p mypool2 put t_pass /etc/passwd</span><br><span class="line"># 查看mypool2 中的文件</span><br><span class="line">[root@ceph_node1 cluster]# rados -p mypool2 ls</span><br><span class="line">t_pass</span><br><span class="line"># 将mypool2中的文件t_pass下载到/tmp/passwd</span><br><span class="line">[root@ceph_node1 cluster]# rados -p mypool2 get t_pass /tmp/passwd</span><br><span class="line"># 查看下载的文件内容</span><br><span class="line">[root@ceph_node1 cluster]# cat /tmp/passwd </span><br><span class="line">root:x:0:0:root:/root:/bin/bash</span><br><span class="line">bin:x:1:1:bin:/bin:/sbin/nologin</span><br></pre></td></tr></table></figure><h2 id="rbd使用"><a href="#rbd使用" class="headerlink" title="rbd使用"></a>rbd使用</h2><p>设置mypool3为rbd，并创建卷，挂载到&#x2F;mnt, 映射给业务服务器使用。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"># 创建存储池mypool3</span><br><span class="line">[root@ceph_node1 cluster]# ceph osd pool create mypool3 64</span><br><span class="line">pool &#x27;mypool3&#x27; created</span><br><span class="line"># 设置存储池类型为rbd</span><br><span class="line">[root@ceph_node1 cluster]# ceph osd pool application enable mypool3 rbd</span><br><span class="line">enabled application &#x27;rbd&#x27; on pool &#x27;mypool3&#x27;</span><br><span class="line"># 在存储池里划分一块名为disk1的磁盘，大小为1G</span><br><span class="line">[root@ceph_node1 cluster]# rbd create mypool3/disk1 --size 1G</span><br><span class="line"># 将创建的磁盘map成一个块设备</span><br><span class="line">[root@ceph_node1 cluster]# rbd map mypool3/disk1</span><br><span class="line">rbd: sysfs write failed</span><br><span class="line">RBD image feature set mismatch. You can disable features unsupported by the kernel with &quot;rbd feature disable mypool3/disk1 object-map fast-diff deep-flatten&quot;.</span><br><span class="line">In some cases useful info is found in syslog - try &quot;dmesg | tail&quot;.</span><br><span class="line">rbd: map failed: (6) No such device or address</span><br><span class="line">[root@ceph_node1 cluster]# rbd feature disable mypool3/disk1 object-map fast-diff deep-flatten</span><br><span class="line">[root@ceph_node1 cluster]# rbd map mypool3/disk1</span><br><span class="line">/dev/rbd0</span><br><span class="line">[root@ceph_node1 cluster]# ll /dev/rbd</span><br><span class="line">rbd/  rbd0  </span><br><span class="line">[root@ceph_node1 cluster]# ll /dev/rbd0</span><br><span class="line">brw-rw----. 1 root disk 252, 0 Feb 18 14:46 /dev/rbd0</span><br><span class="line"># 格式化块设备</span><br><span class="line">[root@ceph_node1 cluster]# mkfs.ext4 /dev/rbd0</span><br><span class="line">mke2fs 1.42.9 (28-Dec-2013)</span><br><span class="line">Discarding device blocks: done                            </span><br><span class="line">Filesystem label=</span><br><span class="line">OS type: Linux</span><br><span class="line">Block size=4096 (log=2)</span><br><span class="line">Fragment size=4096 (log=2)</span><br><span class="line">Stride=1024 blocks, Stripe width=1024 blocks</span><br><span class="line">65536 inodes, 262144 blocks</span><br><span class="line">13107 blocks (5.00%) reserved for the super user</span><br><span class="line">First data block=0</span><br><span class="line">Maximum filesystem blocks=268435456</span><br><span class="line">8 block groups</span><br><span class="line">32768 blocks per group, 32768 fragments per group</span><br><span class="line">8192 inodes per group</span><br><span class="line">Superblock backups stored on blocks: </span><br><span class="line">	32768, 98304, 163840, 229376</span><br><span class="line"></span><br><span class="line">Allocating group tables: done                            </span><br><span class="line">Writing inode tables: done                            </span><br><span class="line">Creating journal (8192 blocks): done</span><br><span class="line">Writing superblocks and filesystem accounting information: done</span><br><span class="line"></span><br><span class="line"># 挂载使用</span><br><span class="line">[root@ceph_node1 cluster]# mount /dev/rbd0 /mnt</span><br><span class="line">[root@ceph_node1 cluster]# df -h</span><br><span class="line">Filesystem               Size  Used Avail Use% Mounted on</span><br><span class="line">devtmpfs                 979M     0  979M   0% /dev</span><br><span class="line">tmpfs                    991M     0  991M   0% /dev/shm</span><br><span class="line">tmpfs                    991M   18M  973M   2% /run</span><br><span class="line">tmpfs                    991M     0  991M   0% /sys/fs/cgroup</span><br><span class="line">/dev/mapper/centos-root   17G  1.9G   16G  11% /</span><br><span class="line">/dev/sda1               1014M  138M  877M  14% /boot</span><br><span class="line">tmpfs                    199M     0  199M   0% /run/user/0</span><br><span class="line">tmpfs                    991M   52K  991M   1% /var/lib/ceph/osd/ceph-0</span><br><span class="line">tmpfs                    991M   52K  991M   1% /var/lib/ceph/osd/ceph-1</span><br><span class="line">tmpfs                    991M   52K  991M   1% /var/lib/ceph/osd/ceph-2</span><br><span class="line">/dev/rbd0                976M  2.6M  907M   1% /mnt</span><br></pre></td></tr></table></figure><h1 id="避坑指南"><a href="#避坑指南" class="headerlink" title="避坑指南"></a>避坑指南</h1><h2 id="Q1"><a href="#Q1" class="headerlink" title="Q1:"></a>Q1:</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">root@ceph_node1 cluster]# ceph-deploy new ceph_node1 ceph_node2 ceph_node3</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;/usr/bin/ceph-deploy&quot;, line 18, in &lt;module&gt;</span><br><span class="line">    from ceph_deploy.cli import main</span><br><span class="line">  File &quot;/usr/lib/python2.7/site-packages/ceph_deploy/cli.py&quot;, line 1, in &lt;module&gt;</span><br><span class="line">    import pkg_resources</span><br><span class="line">ImportError: No module named pkg_resources</span><br></pre></td></tr></table></figure><p>解决方法：<code>yum install python2-pip</code></p><h2 id="Q2："><a href="#Q2：" class="headerlink" title="Q2："></a>Q2：</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">health: HEALTH_WARN</span><br><span class="line">        mons are allowing insecure global_id reclaim</span><br></pre></td></tr></table></figure><p>解决方法：<code>ceph config set mon auth_allow_insecure_global_id_reclaim false</code></p><p>参考地址：<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV15K41137nJ/">https://www.bilibili.com/video/BV15K41137nJ/</a></p></div><footer class="post-footer"><div class="post-tags"><a href="/tags/Ceph/" rel="tag"># Ceph</a></div><div class="post-nav"><div class="post-nav-item"><a href="/arlo/fcd9bd22/" rel="prev" title="简单粗暴：去除小米电视广告"><i class="fa fa-chevron-left"></i> 简单粗暴：去除小米电视广告</a></div><div class="post-nav-item"><a href="/arlo/ef5b34cd/" rel="next" title="好词好句">好词好句 <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><script>window.addEventListener("tabs:register",()=>{let{activeClass:t}=CONFIG.comments;if(CONFIG.comments.storage&&(t=localStorage.getItem("comments_active")||t),t){let e=document.querySelector(`a[href="#comment-${t}"]`);e&&e.click()}}),CONFIG.comments.storage&&window.addEventListener("tabs:click",t=>{if(!t.target.matches(".tabs-comment .tab-content .tab-pane"))return;let e=t.target.classList[1];localStorage.setItem("comments_active",e)})</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Ceph%E4%BB%8B%E7%BB%8D"><span class="nav-number">1.</span> <span class="nav-text">Ceph介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Ceph%E5%9F%BA%E7%A1%80"><span class="nav-number">1.1.</span> <span class="nav-text">Ceph基础</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ceph%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6"><span class="nav-number">1.2.</span> <span class="nav-text">Ceph核心组件</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%A7%84%E5%88%92"><span class="nav-number">2.</span> <span class="nav-text">规划</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%B3%BB%E7%BB%9F%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="nav-number">3.</span> <span class="nav-text">系统初始化</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B3%E9%97%AD%E9%98%B2%E7%81%AB%E5%A2%99"><span class="nav-number">3.1.</span> <span class="nav-text">关闭防火墙</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B3%E9%97%ADSELinux"><span class="nav-number">3.2.</span> <span class="nav-text">关闭SELinux</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%85%8D%E7%BD%AE%E7%BD%91%E7%BB%9C"><span class="nav-number">3.3.</span> <span class="nav-text">配置网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%85%8D%E7%BD%AE%E4%B8%BB%E6%9C%BA%E5%90%8D"><span class="nav-number">3.4.</span> <span class="nav-text">配置主机名</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%85%8D%E7%BD%AEhosts"><span class="nav-number">3.5.</span> <span class="nav-text">配置hosts</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%85%8D%E7%BD%AE%E4%BA%92%E4%BF%A1"><span class="nav-number">3.6.</span> <span class="nav-text">配置互信</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%85%8D%E7%BD%AE%E6%97%B6%E9%97%B4%E6%9C%8D%E5%8A%A1%E5%99%A8"><span class="nav-number">3.7.</span> <span class="nav-text">配置时间服务器</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%85%8D%E7%BD%AEyum%E6%BA%90"><span class="nav-number">3.8.</span> <span class="nav-text">配置yum源</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E7%A1%80yum%E6%BA%90"><span class="nav-number">3.8.1.</span> <span class="nav-text">基础yum源</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#epel%E6%BA%90"><span class="nav-number">3.8.2.</span> <span class="nav-text">epel源</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ceph%E6%BA%90"><span class="nav-number">3.8.3.</span> <span class="nav-text">ceph源</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%83%A8%E7%BD%B2Ceph"><span class="nav-number">4.</span> <span class="nav-text">部署Ceph</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%83%A8%E7%BD%B2%E6%8E%A7%E5%88%B6%E8%8A%82%E7%82%B9"><span class="nav-number">4.1.</span> <span class="nav-text">部署控制节点</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%94%9F%E6%88%90mon%E8%A7%92%E8%89%B2"><span class="nav-number">4.2.</span> <span class="nav-text">生成mon角色</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%94%9F%E6%88%90ceph-admin%E7%A7%98%E9%92%A5"><span class="nav-number">4.3.</span> <span class="nav-text">生成ceph admin秘钥</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%83%A8%E7%BD%B2MGR%EF%BC%8C%E6%8F%90%E4%BE%9Bweb%E7%95%8C%E9%9D%A2%E7%AE%A1%E7%90%86ceph%EF%BC%88%E5%8F%AF%E9%80%89%E5%AE%89%E8%A3%85%EF%BC%89"><span class="nav-number">4.4.</span> <span class="nav-text">部署MGR，提供web界面管理ceph（可选安装）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%83%A8%E7%BD%B2rgw"><span class="nav-number">4.5.</span> <span class="nav-text">部署rgw</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%83%A8%E7%BD%B2cephfs%EF%BC%88%E5%8F%AF%E9%80%89%EF%BC%89"><span class="nav-number">4.6.</span> <span class="nav-text">部署cephfs（可选）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96OSD"><span class="nav-number">4.7.</span> <span class="nav-text">初始化OSD</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9F%A5%E7%9C%8BOSD%E7%8A%B6%E6%80%81"><span class="nav-number">4.8.</span> <span class="nav-text">查看OSD状态</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9F%A5%E7%9C%8Bceph%E7%8A%B6%E6%80%81"><span class="nav-number">4.9.</span> <span class="nav-text">查看ceph状态</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Ceph%E6%93%8D%E4%BD%9C%E4%BD%BF%E7%94%A8"><span class="nav-number">5.</span> <span class="nav-text">Ceph操作使用</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA%E5%AD%98%E5%82%A8%E6%B1%A0"><span class="nav-number">5.1.</span> <span class="nav-text">创建存储池</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9F%A5%E7%9C%8B%E5%AD%98%E5%82%A8%E6%B1%A0%E8%AF%A6%E7%BB%86%E5%8F%82%E6%95%B0"><span class="nav-number">5.2.</span> <span class="nav-text">查看存储池详细参数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9F%A5%E7%9C%8B%E5%AD%98%E5%82%A8%E6%B1%A0%E5%8D%95%E4%B8%AA%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE"><span class="nav-number">5.3.</span> <span class="nav-text">查看存储池单个参数配置</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BF%AE%E6%94%B9%E5%AD%98%E5%82%A8%E6%B1%A0%E5%8F%82%E6%95%B0"><span class="nav-number">5.4.</span> <span class="nav-text">修改存储池参数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%9B%E5%BB%BAEC%E6%B1%A0%EF%BC%8C%E5%88%9B%E5%BB%BAEC%E7%AD%96%E7%95%A5"><span class="nav-number">5.5.</span> <span class="nav-text">创建EC池，创建EC策略</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#rgw%E4%BD%BF%E7%94%A8"><span class="nav-number">5.6.</span> <span class="nav-text">rgw使用</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#rbd%E4%BD%BF%E7%94%A8"><span class="nav-number">5.7.</span> <span class="nav-text">rbd使用</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%81%BF%E5%9D%91%E6%8C%87%E5%8D%97"><span class="nav-number">6.</span> <span class="nav-text">避坑指南</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Q1"><span class="nav-number">6.1.</span> <span class="nav-text">Q1:</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Q2%EF%BC%9A"><span class="nav-number">6.2.</span> <span class="nav-text">Q2：</span></a></li></ol></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="南山小樵" src="/images/avatar.jpg"><p class="site-author-name" itemprop="name">南山小樵</p><div class="site-description" itemprop="description"></div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">174</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">17</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">101</span> <span class="site-state-item-name">标签</span></a></div></nav></div></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2023</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">南山小樵</span></div><div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动</div></div></footer></div><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script></body></html>